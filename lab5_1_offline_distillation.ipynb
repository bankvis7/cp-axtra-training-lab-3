{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2pEJiuLZ-7b"
      },
      "source": [
        "# LLM Distillation: Offline Distillation\n",
        "\n",
        "<div class=\"align-center\">\n",
        "\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a>\n",
        "</div>\n",
        "\n",
        "## วัตถุประสงค์การเรียนรู้\n",
        "\n",
        "\n",
        "1. เข้าใจว่า **offline distillation** คืออะไรและแตกต่างจาก fine-tuning ปกติอย่างไร\n",
        "2. สร้างชุดข้อมูลสำหรับ training จาก **teacher model**\n",
        "3. ฝึก **student model** ขนาดเล็กกว่าให้เลียนแบบ output ของ teacher\n",
        "4. ใช้ **Unsloth + QLoRA** สำหรับการ training student อย่างมีประสิทธิภาพ\n",
        "5. เปรียบเทียบประสิทธิภาพและความเร็วระหว่าง student กับ teacher\n",
        "6. ศึกษาโมเดล distilled ระดับ production (DeepSeek-R1-Distill)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWcUcMPZ-7c"
      },
      "source": [
        "## Knowledge Distillation คืออะไร?\n",
        "\n",
        "**Knowledge Distillation** เป็นการถ่ายโอนความรู้จาก **teacher model** ขนาดใหญ่ไปยัง **student model** ขนาดเล็กกว่า\n",
        "\n",
        "### กระบวนการ Offline Distillation:\n",
        "\n",
        "```\n",
        "1. Teacher Model → สร้าง responses สำหรับข้อมูล training\n",
        "2. บันทึก teacher outputs (offline dataset)\n",
        "3. Student Model → เรียนรู้เพื่อเลียนแบบ responses ของ teacher\n",
        "4. Deploy student model ที่เล็กกว่าและเร็วกว่า\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTkpnR53Z-7c"
      },
      "source": [
        "## ส่วนที่ 1: การติดตั้ง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_iKiIfHZ-7d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Colab-specific installation\n",
        "    import torch; v = re.match(r\"[0-9\\\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBA5CwmrZ-7d"
      },
      "outputs": [],
      "source": [
        "# ตรวจสอบ GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoI3HjC5Z-7d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline\n",
        ")\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFOHOMeLZ-7d"
      },
      "source": [
        "## ส่วนที่ 2: โหลดชุดข้อมูล Instruction\n",
        "\n",
        "เราจะใช้ชุดข้อมูล **Alpaca** ที่มีตัวอย่าง instruction-following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McQtuVUdZ-7d"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"กำลังโหลดชุดข้อมูล ALPACA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# โหลดชุดข้อมูล Alpaca\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "# ใช้ชุดข้อมูลย่อยเพื่อความรวดเร็ว (300 ตัวอย่าง)\n",
        "dataset = dataset.shuffle(seed=42).select(range(300))\n",
        "\n",
        "print(f\"\\nขนาดชุดข้อมูล: {len(dataset):,} ตัวอย่าง\")\n",
        "print(\"\\nตัวอย่างจากชุดข้อมูล:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Instruction: {dataset[0]['instruction']}\")\n",
        "print(f\"Input: {dataset[0]['input']}\")\n",
        "print(f\"Output: {dataset[0]['output']}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXJ7kx12Z-7d"
      },
      "outputs": [],
      "source": [
        "def format_instruction(example):\n",
        "    \"\"\"จัดรูปแบบชุดข้อมูลเป็น instruction prompts\"\"\"\n",
        "    if example[\"input\"].strip():\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Input:\n",
        "{example['input']}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# แสดงตัวอย่าง formatted prompt\n",
        "print(\"ตัวอย่าง formatted prompt:\")\n",
        "print(\"=\"*60)\n",
        "print(format_instruction(dataset[0]))\n",
        "print(f\"Output ที่คาดหวัง: {dataset[0]['output']}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCbd2kSZ-7e"
      },
      "source": [
        "## ส่วนที่ 3: โหลด Teacher Model\n",
        "\n",
        "เราจะใช้ **Gemma-2-2B-Instruct** เป็น teacher model -\n",
        "\n",
        "ในการใช้งานจริง teachers มักจะใหญ่กว่ามาก (7B, 70B, 405B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjWRQ4EAZ-7e"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังโหลด TEACHER MODEL: Gemma-2-2B-Instruct\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "teacher_model_name = \"unsloth/gemma-2-2b-it\"\n",
        "\n",
        "print(f\"กำลังโหลด teacher model: {teacher_model_name}\")\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    teacher_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ โหลด teacher model เรียบร้อย: Gemma-2-2B (2.5B พารามิเตอร์)\")\n",
        "print(f\"  Model dtype: {teacher_model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y9IogxRZ-7e"
      },
      "source": [
        "## ส่วนที่ 4: สร้าง Teacher Outputs (Offline Distillation Dataset)\n",
        "\n",
        "นี่คือส่วน **\"offline\"** - เราสร้าง teacher responses ครั้งเดียวแล้วบันทึก\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NGy2ywyZ-7e"
      },
      "outputs": [],
      "source": [
        "# สร้าง teacher pipeline\n",
        "teacher_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=teacher_model,\n",
        "    tokenizer=teacher_tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# ทดสอบกับตัวอย่างหนึ่ง\n",
        "test_prompt = format_instruction(dataset[0])\n",
        "test_output = teacher_pipe(test_prompt)[0]['generated_text']\n",
        "\n",
        "print(\"Teacher test output:\")\n",
        "print(\"=\"*60)\n",
        "print(test_output)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxnUO7m2Z-7e"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังสร้าง TEACHER OUTPUTS สำหรับ DISTILLATION\")\n",
        "print(\"=\"*60)\n",
        "teacher_outputs = []\n",
        "batch_size = 8\n",
        "\n",
        "for i in tqdm(range(0, len(dataset), batch_size), desc=\"กำลังสร้าง teacher outputs\"):\n",
        "    batch_indices = range(i, min(i+batch_size, len(dataset)))\n",
        "    prompts = [format_instruction(dataset[idx]) for idx in batch_indices]\n",
        "\n",
        "    # สร้าง teacher responses\n",
        "    outputs = teacher_pipe(prompts, batch_size=batch_size)\n",
        "\n",
        "    for prompt, output in zip(prompts, outputs):\n",
        "        generated_text = output[0]['generated_text']\n",
        "        # ดึงเฉพาะส่วน response (หลังจาก prompt)\n",
        "        response = generated_text[len(prompt):].strip()\n",
        "        teacher_outputs.append({\n",
        "            'prompt': prompt,\n",
        "            'teacher_response': response\n",
        "        })\n",
        "\n",
        "print(f\"\\n✓ สร้าง teacher outputs แล้ว {len(teacher_outputs):,} ตัวอย่าง\")\n",
        "print(\"\\nตัวอย่าง teacher output:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prompt: {teacher_outputs[0]['prompt'][:100]}...\")\n",
        "print(f\"Teacher Response: {teacher_outputs[0]['teacher_response']}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ScGFuUzZ-7f"
      },
      "outputs": [],
      "source": [
        "# บันทึก teacher outputs สำหรับใช้ในอนาคต\n",
        "with open('teacher_outputs_distillation.json', 'w') as f:\n",
        "    json.dump(teacher_outputs, f, indent=2)\n",
        "\n",
        "print(\"✓ บันทึก teacher outputs ไปที่ 'teacher_outputs_distillation.json'\")\n",
        "\n",
        "# ปล่อยหน่วยความจำของ teacher model\n",
        "del teacher_model\n",
        "del teacher_pipe\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"✓ ถอด teacher model เพื่อปล่อยหน่วยความจำ GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5QVfQk7Z-7f"
      },
      "source": [
        "## ส่วนที่ 5: เตรียมชุดข้อมูล Student Training\n",
        "\n",
        "แปลง teacher outputs เป็นรูปแบบ training สำหรับ student model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDlkIsSiZ-7f"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังเตรียมชุดข้อมูล DISTILLATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# สร้าง training texts: prompt + teacher response\n",
        "training_data = []\n",
        "for item in teacher_outputs:\n",
        "    # Student เรียนรู้เพื่อสร้าง response ของ teacher จาก prompt\n",
        "    full_text = item['prompt'] + item['teacher_response']\n",
        "    training_data.append({'text': full_text})\n",
        "\n",
        "distillation_dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# แบ่งเป็น train/test\n",
        "train_size = int(0.9 * len(distillation_dataset))\n",
        "train_dataset = distillation_dataset.select(range(train_size))\n",
        "test_dataset = distillation_dataset.select(range(train_size, len(distillation_dataset)))\n",
        "\n",
        "print(f\"\\n✓ สร้างชุดข้อมูล distillation เรียบร้อย\")\n",
        "print(f\"  ตัวอย่าง training: {len(train_dataset):,}\")\n",
        "print(f\"  ตัวอย่าง test: {len(test_dataset):,}\")\n",
        "print(\"\\nแสดงตัวอย่าง training:\")\n",
        "print(\"=\"*60)\n",
        "print(train_dataset[0]['text'][:500] + \"...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A73Cb7CIZ-7f"
      },
      "source": [
        "## ส่วนที่ 6: โหลด Student Model ด้วย Unsloth\n",
        "\n",
        "เราจะใช้ **Qwen2.5-0.5B** เป็น student - เล็กกว่า teacher 5 เท่า!\n",
        "\n",
        "- **Teacher**: Gemma-2-2B (2.5B พารามิเตอร์)\n",
        "- **Student**: Qwen2.5-0.5B (0.5B พารามิเตอร์)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icOqHqkbZ-7f"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังโหลด STUDENT MODEL: Qwen2.5-0.5B\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ล้างหน่วยความจำ GPU\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "student_model, student_tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-0.5B\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,  # ใช้ 4-bit quantization เพื่อประสิทธิภาพ\n",
        "    dtype = None,  # Auto-detect\n",
        ")\n",
        "\n",
        "base_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\n✓ โหลด student model เรียบร้อย: Qwen2.5-0.5B (0.5B พารามิเตอร์)\")\n",
        "print(f\"  หน่วยความจำ base model: {base_memory} GB\")\n",
        "print(f\"  Model dtype: {student_model.dtype}\")\n",
        "print(f\"\\n  การเปรียบเทียบขนาด: Student เล็กกว่า ~5 เท่า!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTtCs9_pZ-7f"
      },
      "source": [
        "## ส่วนที่ 7: เพิ่ม LoRA Adapters ให้ Student\n",
        "\n",
        "เราจะใช้ QLoRA เพื่อฝึก student model อย่างมีประสิทธิภาพ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZblvnQ46Z-7f"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังเพิ่ม LoRA ADAPTERS ให้ STUDENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "student_model = FastModel.get_peft_model(\n",
        "    student_model,\n",
        "    r = 16,  # LoRA rank\n",
        "    lora_alpha = 16,  # LoRA scaling\n",
        "    lora_dropout = 0,  # Unsloth optimized\n",
        "\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# นับพารามิเตอร์ที่ฝึกได้\n",
        "trainable_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in student_model.parameters())\n",
        "trainable_percent = 100 * trainable_params / all_params\n",
        "\n",
        "print(f\"\\n✓ เพิ่ม LoRA adapters เรียบร้อย\")\n",
        "print(f\"  พารามิเตอร์ที่ฝึกได้: {trainable_params:,}\")\n",
        "print(f\"  พารามิเตอร์ทั้งหมด: {all_params:,}\")\n",
        "print(f\"  เปอร์เซ็นต์ที่ฝึกได้: {trainable_percent:.2f}%\")\n",
        "\n",
        "lora_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\n  หน่วยความจำพร้อม LoRA: {lora_memory} GB\")\n",
        "print(f\"  หน่วยความจำเพิ่มเติมสำหรับ LoRA: {lora_memory - base_memory:.3f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADGjgP4CZ-7f"
      },
      "source": [
        "## ส่วนที่ 8: กำหนดค่าการ Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0r4UnbnZ-7f"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังกำหนดค่าการ TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = student_model,\n",
        "    tokenizer = student_tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = student_tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir = \"distilled_student_outputs\",\n",
        "\n",
        "        # ระยะเวลา training (ปรับแต่งสำหรับ 4-5 นาที)\n",
        "        num_train_epochs = 1,\n",
        "        max_steps = -1,\n",
        "\n",
        "        # การตั้งค่า batch\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2,  # Effective batch size = 8\n",
        "\n",
        "        # Learning rate\n",
        "        learning_rate = 2e-4,\n",
        "        warmup_steps = 10,\n",
        "\n",
        "        # Optimization\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "\n",
        "        # Precision\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "\n",
        "        # Logging และ saving\n",
        "        logging_steps = 10,\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 50,\n",
        "        save_strategy = \"epoch\",\n",
        "        save_total_limit = 1,\n",
        "\n",
        "        # Misc\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\n กำหนดค่า training สำหรับ distillation อย่างรวดเร็ว\")\n",
        "print(f\"  ตัวอย่าง training: {len(train_dataset):,}\")\n",
        "print(f\"  Effective batch size: 8\")\n",
        "print(f\"  เวลา training โดยประมาณ: 4-5 นาที\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ4P88VUZ-7g"
      },
      "source": [
        "## ส่วนที่ 9: ฝึก Student Model (Distillation)\n",
        "\n",
        "ถึงเวลาที่จะ distill ความรู้ของ teacher เข้าไปใน student!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c5xydnPZ-7g"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"เริ่มการ DISTILLATION TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(\"Student กำลังเรียนรู้เพื่อเลียนแบบ responses ของ teacher...\\n\")\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "peak_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"การ DISTILLATION TRAINING เสร็จสมบูรณ์!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"เวลา training: {training_time/60:.2f} นาที\")\n",
        "print(f\"หน่วยความจำ GPU สูงสุด: {peak_memory} GB\")\n",
        "print(f\"Training loss สุดท้าย: {trainer_stats.training_loss:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7Mw-lCZ-7g"
      },
      "source": [
        "## ส่วนที่ 10: ทดสอบ Distilled Student Model\n",
        "\n",
        "มาเปรียบเทียบ distilled student กับตัวอย่างชุดข้อมูลเดิม"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4LzSz2SZ-7g"
      },
      "outputs": [],
      "source": [
        "def test_student(model, tokenizer, prompt, max_tokens=200):\n",
        "    \"\"\"สร้าง response จาก student model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = max_tokens,\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.9,\n",
        "        do_sample = True,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# เลือกตัวอย่างทดสอบ\n",
        "test_indices = [10, 25, 50]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังทดสอบ DISTILLED STUDENT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for idx in test_indices:\n",
        "    example = dataset[idx]\n",
        "    prompt = format_instruction(example)\n",
        "\n",
        "    print(f\"\\n[ตัวอย่างที่ {idx}]\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"Instruction: {example['instruction']}\")\n",
        "    if example['input'].strip():\n",
        "        print(f\"Input: {example['input']}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    # สร้าง student response\n",
        "    student_response = test_student(student_model, student_tokenizer, prompt)\n",
        "\n",
        "    print(f\"Student Response (Distilled):\\n{student_response}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"Teacher Response (จาก Dataset):\\n{teacher_outputs[idx]['teacher_response']}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"Ground Truth (ต้นฉบับ):\\n{example['output']}\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hFeAV_0Z-7g"
      },
      "source": [
        "## ส่วนที่ 11: เปรียบเทียบ Student vs Teacher\n",
        "\n",
        "มาเปรียบเทียบขนาดโมเดล ความเร็ว และการใช้หน่วยความจำ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJRXn_PdZ-7g"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"การเปรียบเทียบโมเดล: TEACHER vs STUDENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_data = {\n",
        "    'ตัวชี้วัด': [\n",
        "        'โมเดล',\n",
        "        'พารามิเตอร์',\n",
        "        'อัตราส่วนขนาด',\n",
        "        'เวลา Training',\n",
        "        'การใช้หน่วยความจำ',\n",
        "        'ความเร็ว Inference (โดยประมาณ)',\n",
        "        'กรณีการใช้งาน'\n",
        "    ],\n",
        "    'Teacher (Gemma-2-2B)': [\n",
        "        'Gemma-2-2B-Instruct',\n",
        "        '2.5B',\n",
        "        '1x (baseline)',\n",
        "        'N/A (pre-trained)',\n",
        "        '~5-6 GB',\n",
        "        '1x (baseline)',\n",
        "        'สร้างข้อมูล training'\n",
        "    ],\n",
        "    'Student (Qwen2.5-0.5B)': [\n",
        "        'Qwen2.5-0.5B + LoRA',\n",
        "        '0.5B',\n",
        "        'เล็กกว่า 5 เท่า',\n",
        "        f'{training_time/60:.1f} นาที',\n",
        "        f'{peak_memory} GB',\n",
        "        '~เร็วกว่า 3-5 เท่า',\n",
        "        'การ deploy ในระบบจริง'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\", df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"\\nข้อดีหลักของ Distilled Student:\")\n",
        "print(\"  ✓ ขนาดโมเดลเล็กกว่า 5 เท่า\")\n",
        "print(\"  ✓ Inference เร็วกว่า 3-5 เท่า\")\n",
        "print(\"  ✓ ใช้หน่วยความจำน้อยกว่า\")\n",
        "print(\"  ✓ เรียนรู้ความรู้ของ teacher\")\n",
        "print(\"  ✓ เหมาะสำหรับ edge/mobile deployment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yDcyrOmZ-7h"
      },
      "source": [
        "## ส่วนที่ 12: บันทึก Distilled Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2ttVIQlZ-7h"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังบันทึก DISTILLED STUDENT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# บันทึกเฉพาะ LoRA adapters (ขนาดเล็ก)\n",
        "student_model.save_pretrained(\"distilled_student_lora\")\n",
        "student_tokenizer.save_pretrained(\"distilled_student_lora\")\n",
        "print(\"\\n✓ บันทึก LoRA adapters ไปที่: distilled_student_lora/\")\n",
        "\n",
        "# ตรวจสอบขนาด adapter\n",
        "import os\n",
        "adapter_path = \"distilled_student_lora/adapter_model.safetensors\"\n",
        "if os.path.exists(adapter_path):\n",
        "    size_mb = os.path.getsize(adapter_path) / (1024 * 1024)\n",
        "    print(f\"  ขนาด adapter: {size_mb:.1f} MB\")\n",
        "\n",
        "# บันทึก merged model (เสริม)\n",
        "student_model.save_pretrained_merged(\n",
        "    \"distilled_student_merged\",\n",
        "    student_tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        ")\n",
        "print(\"\\n✓ บันทึก merged model ไปที่: distilled_student_merged/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntZXV0HxZ-7h"
      },
      "source": [
        "## ส่วนที่ 13: ทดลองโมเดล Distilled ระดับ Production\n",
        "\n",
        "มาทดสอบ **DeepSeek-R1-Distill-Qwen-1.5B** - โมเดล distilled ระดับ production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYhAq7KjZ-7i"
      },
      "outputs": [],
      "source": [
        "# ล้าง student model\n",
        "del student_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังโหลดโมเดล DISTILLED ระดับ PRODUCTION\")\n",
        "print(\"DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "deepseek_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "deepseek_tokenizer = AutoTokenizer.from_pretrained(deepseek_model_name)\n",
        "deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
        "    deepseek_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ โหลดโมเดลแล้ว: {deepseek_model_name}\")\n",
        "print(f\"  พารามิเตอร์: ~1.5B\")\n",
        "print(\"\\nคุณสมบัติหลัก:\")\n",
        "print(\"  - Distilled จาก DeepSeek-R1 (reasoning model ที่ใหญ่กว่ามาก)\")\n",
        "print(\"  - รักษาความสามารถด้าน reasoning ที่แข็งแกร่ง\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciyC_1s3Z-7i"
      },
      "outputs": [],
      "source": [
        "# สร้าง pipeline\n",
        "deepseek_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=deepseek_model,\n",
        "    tokenizer=deepseek_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(\" DeepSeek pipeline พร้อม\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pVfW32KZ-7i"
      },
      "source": [
        "### ทดสอบ DeepSeek กับงาน Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15sug9eAZ-7i"
      },
      "outputs": [],
      "source": [
        "# Test with reasoning tasks\n",
        "reasoning_tests = [\n",
        "    {\n",
        "        \"name\": \"Math Reasoning\",\n",
        "        \"prompt\": \"\"\"Solve this problem step by step:\n",
        "\n",
        "A store sells notebooks for $3 each and pens for $2 each. If you buy 5 notebooks and 8 pens, how much will you pay in total?\n",
        "\n",
        "Answer:\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Logic Puzzle\",\n",
        "        \"prompt\": \"\"\"Answer this logic puzzle:\n",
        "\n",
        "If all cats are animals, and some animals are pets, can we conclude that all cats are pets?\n",
        "\n",
        "Explain your reasoning:\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Code Generation\",\n",
        "        \"prompt\": \"\"\"Write a Python function to check if a string is a palindrome.\n",
        "\n",
        "Function:\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"กำลังทดสอบโมเดล DISTILLED ระดับ PRODUCTION (DeepSeek)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for test in reasoning_tests:\n",
        "    print(f\"\\n[{test['name']}]\")\n",
        "    print(\"-\"*60)\n",
        "    print(test['prompt'])\n",
        "    print(\"\\nDeepSeek Response:\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    response = deepseek_pipe(test['prompt'], max_new_tokens=300)[0]['generated_text']\n",
        "    print(response[len(test['prompt']):])\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRvgzEpJZ-7i"
      },
      "source": [
        "## ส่วนที่ 14: การเปรียบเทียบประสิทธิภาพ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7eficgkZ-7i"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "test_prompts_perf = [\n",
        "    \"เมืองหลวงของฝรั่งเศสคือที่ไหน?\",\n",
        "    \"อธิบาย machine learning ในประโยคเดียว\",\n",
        "    \"เขียน haiku เกี่ยวกับการเขียนโค้ด\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"การทดสอบความเร็ว INFERENCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_time = 0\n",
        "for prompt in test_prompts_perf:\n",
        "    start = time.time()\n",
        "    _ = deepseek_pipe(prompt, max_new_tokens=50)[0]['generated_text']\n",
        "    elapsed = time.time() - start\n",
        "    total_time += elapsed\n",
        "    print(f\"Prompt: {prompt[:40]:40s} | เวลา: {elapsed:.2f}s\")\n",
        "\n",
        "avg_time = total_time / len(test_prompts_perf)\n",
        "print(f\"\\nเวลา inference เฉลี่ย: {avg_time:.2f}s\")\n",
        "print(f\"ขนาดโมเดล: ~1.5B พารามิเตอร์\")\n",
        "print(f\"การใช้หน่วยความจำ: ~3-4 GB VRAM (FP16)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sp3KiNsfptjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}