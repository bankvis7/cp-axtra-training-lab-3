{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnopbR2RUkhd"
      },
      "source": [
        "# Knowledge Distillation for HuggingFace Models - Single GPU\n",
        "\n",
        "This notebook demonstrates how to perform **Knowledge Distillation** using TensorRT Model Optimizer on a single GPU (perfect for Google Colab).\n",
        "\n",
        "## What is Knowledge Distillation?\n",
        "\n",
        "Knowledge Distillation is a technique where a smaller \"student\" model learns to mimic a larger \"teacher\" model. The student learns from both:\n",
        "1. **Ground truth labels** (standard training)\n",
        "2. **Soft predictions from the teacher** (distillation)\n",
        "\n",
        "### Example in this notebook:\n",
        "- **Teacher**: Llama-3.2-3B-Instruct (3 billion parameters)\n",
        "- **Student**: Llama-3.2-1B (1 billion parameters)\n",
        "- **Result**: A smaller, faster model with teacher's knowledge!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx2N3INRUkhf"
      },
      "source": [
        "## üì¶ Step 1: Install Dependencies\n",
        "\n",
        "First, we need to install TensorRT Model Optimizer and other required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9IlAEG6Ukhg",
        "outputId": "c48a12d9-3d22-4abc-9a2d-02ebf3199a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.9.5)\n",
            "Requirement already satisfied: nvitop in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: nvidia-ml-py<13.581.0a0,>=11.450.51 in /usr/local/lib/python3.12/dist-packages (from nvitop) (13.580.82)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.12/dist-packages (from nvitop) (7.1.2)\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 607ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 0.53ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 343ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 253ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 263ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m68 packages\u001b[0m \u001b[2min 71ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 105ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# For faster library installation\n",
        "!pip install uv nvitop\n",
        "# Install TensorRT Model Optimizer with HuggingFace support\n",
        "!uv pip install -U nvidia-modelopt[hf]\n",
        "\n",
        "!uv pip uninstall numpy transformers\n",
        "# Install additional dependencies\n",
        "!uv pip install pyarrow 'transformers<5.0' 'trl>=0.23.0' 'numpy<2.0' bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHYpAvdrUkhh"
      },
      "source": [
        "## üîß Step 2: Check GPU Availability\n",
        "\n",
        "Let's verify that we have a GPU available for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72-RlqWRUkhh",
        "outputId": "87cca8ba-a251-4f05-9e50-b70826943779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 27 12:00:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49JaUImWUkhh"
      },
      "source": [
        "## üìö Step 3: Import Libraries\n",
        "\n",
        "Import all necessary libraries for knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9y6JLFrUkhh",
        "outputId": "334c8363-d738-4243-ffb3-59564dd5b7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All libraries imported successfully!\n",
            "==> PyTorch version: 2.9.0+cu128\n",
            "==> Transformers version: 4.57.1\n",
            "==> CUDA available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/modelopt/torch/__init__.py:36: UserWarning: transformers version 4.57.1 is incompatible with nvidia-modelopt and may cause issues. Please install recommended version with `pip install nvidia-modelopt[hf]` if working with HF models.\n",
            "  _warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Optimize CUDA memory allocation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# TensorRT Model Optimizer imports\n",
        "import modelopt.torch.distill as mtd\n",
        "import modelopt.torch.opt as mto\n",
        "from modelopt.torch.distill.plugins.huggingface import KDTrainer, LMLogitsLoss\n",
        "\n",
        "print(\"‚úì All libraries imported successfully!\")\n",
        "print(f\"==> PyTorch version: {torch.__version__}\")\n",
        "print(f\"==> Transformers version: {transformers.__version__}\")\n",
        "print(f\"==> CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PO1hCLHUkhi"
      },
      "source": [
        "## ‚öôÔ∏è Step 4: Configuration\n",
        "\n",
        "Set up the configuration for models and training hyperparameters.\n",
        "\n",
        "### üìù You can modify these settings:\n",
        "- **Models**: Change teacher/student models\n",
        "- **Batch size**: Adjust based on your GPU memory\n",
        "- **Training steps**: Increase for better results (will take longer)\n",
        "- **Learning rate**: Fine-tune the learning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTBCxOK_Ukhi",
        "outputId": "ec3562ab-4ea7-47c4-efc6-39bd6fad1f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  Teacher: meta-llama/Llama-3.2-3B-Instruct\n",
            "  Student: meta-llama/Llama-3.2-1B\n",
            "  Batch size: 1\n",
            "  Gradient accumulation: 1\n",
            "  Effective batch size: 1\n",
            "  Max steps: 200\n",
            "  Learning rate: 1e-05\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"Model Configuration\"\"\"\n",
        "    # Teacher: Larger model we distill FROM\n",
        "    teacher_name_or_path: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "    # Student: Smaller model we distill TO\n",
        "    student_name_or_path: str = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    \"\"\"Training Configuration\"\"\"\n",
        "    output_dir: str = \"./llama3.2-1b-distilled\"\n",
        "    do_train: bool = True\n",
        "    do_eval: bool = True\n",
        "    save_strategy: str = \"steps\"\n",
        "    save_steps: int = 100\n",
        "    max_length: int = 512\n",
        "\n",
        "    # Optimizer settings\n",
        "    optim: str = \"adamw_torch\"\n",
        "    learning_rate: float = 1e-5\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "\n",
        "    # Data processing\n",
        "    dataloader_drop_last: bool = True\n",
        "    dataset_num_proc: int = 4\n",
        "\n",
        "    # Mixed precision (faster training, less memory)\n",
        "    bf16: bool = True\n",
        "    tf32: bool = False\n",
        "\n",
        "    # Batch size - ADJUST based on your GPU memory!\n",
        "    per_device_train_batch_size: int = 1\n",
        "    per_device_eval_batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 1  # Effective batch size = 1 * 4 = 4\n",
        "\n",
        "    # Training duration\n",
        "    max_steps: int = 200  # Increase for better results (e.g., 500, 1000)\n",
        "\n",
        "    # Logging\n",
        "    logging_steps: int = 5\n",
        "    eval_steps: int = 50\n",
        "    warmup_steps: int = 10\n",
        "    report_to: str = \"none\" # Disable wandb reporting\n",
        "\n",
        "\n",
        "# Create configuration instances\n",
        "model_args = ModelArguments()\n",
        "training_args = TrainingArguments(output_dir=\"./llama3.2-1b-distilled\")\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Teacher: {model_args.teacher_name_or_path}\")\n",
        "print(f\"  Student: {model_args.student_name_or_path}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Max steps: {training_args.max_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2xH9frzUkhi"
      },
      "source": [
        "## üìä Step 5: Load Dataset\n",
        "\n",
        "We'll use the **smol-smoltalk-Interaction-SFT** dataset, which contains conversational query-answer pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmTr7Bk5Ukhi",
        "outputId": "7be3baf9-d2df-4f7d-d94b-0f2f32750b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dataset loaded!\n",
            "  Training samples: 12,800\n",
            "  Evaluation samples: 1,280\n",
            "\n",
            "Sample data:\n",
            "{'query': 'What are Data visualization types.', 'answer': 'Data visualization types are diverse and can be categorized based on their purpose, structure, and functionality. Here are some common data visualization types:\\n\\n**Basic Visualization Types:**\\n\\n1. Bar charts: Used to compare categorical data across different groups.\\n2. Line charts: Used to show trends and patterns over time.\\n3. Pie charts: Used to represent proportional data.\\n4. Histograms: Used to display the distribution of continuous data.\\n5. Scatter plots: Used to visualize relationships between two variables.\\n\\n**Advanced Visualization Types:**\\n\\n1. Heat maps: Used to display complex relationships between two variables.\\n2. Tree maps: Used to display hierarchical data.\\n3. Network diagrams: Used to show relationships between entities.\\n4. Sankey diagrams: Used to display flows and relationships between variables.\\n5. Gauge charts: Used to display progress towards a goal or target.\\n\\n**Interactive Visualization Types:**\\n\\n1. Dashboards: Used to display multiple visualizations and metrics in a single view.\\n2. Storyboards: Used to present a sequence of visualizations to tell a story.\\n3. Drill-down charts: Used to provide more detailed information on a specific data point.\\n4. Zoomable charts: Used to explore data at different levels of granularity.\\n5. Interactive filters: Used to filter data based on user input.\\n\\n**Geospatial Visualization Types:**\\n\\n1. Maps: Used to display geographic data, such as country or region-specific data.\\n2. Geospatial heat maps: Used to display the density of data points on a map.\\n3. Choropleth maps: Used to display categorical data on a map.\\n4. Bubble maps: Used to display quantitative data on a map.\\n\\n**3D and Animated Visualization Types:**\\n\\n1. 3D bar charts: Used to display complex relationships between variables.\\n2. 3D scatter plots: Used to visualize relationships between three variables.\\n3. Animated charts: Used to display data over time or to show changes in data.\\n4. Simulation visualizations: Used to display complex systems or processes.\\n\\nThese are just a few examples of the many data visualization types available. The choice of visualization type depends on the data, the message, and the audience.', 'source': 'smol-magpie-ultra-short'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading dataset...\")\n",
        "\n",
        "# Load the dataset from HuggingFace\n",
        "dset = datasets.load_dataset(\"ReactiveAI/smol-smoltalk-Interaction-SFT\", split=\"train\")\n",
        "\n",
        "# Split into training and evaluation sets\n",
        "dset_splits = dset.train_test_split(train_size=12800, test_size=1280, seed=420)\n",
        "dset_train, dset_eval = dset_splits[\"train\"], dset_splits[\"test\"]\n",
        "\n",
        "print(f\"‚úì Dataset loaded!\")\n",
        "print(f\"  Training samples: {len(dset_train):,}\")\n",
        "print(f\"  Evaluation samples: {len(dset_eval):,}\")\n",
        "print(f\"\\nSample data:\")\n",
        "print(dset_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89mNqWAUkhj"
      },
      "source": [
        "## üî§ Step 6: Load Tokenizer\n",
        "\n",
        "Load the tokenizer to convert text into tokens that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv_g1dTlUkhj",
        "outputId": "78a0ccaa-dabf-4e69-809a-7f7168bb7adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "‚úì Successfully logged in to Hugging Face Hub!\n",
            "‚úì Tokenizer loaded from meta-llama/Llama-3.2-3B-Instruct\n",
            "  Vocab size: 128,256\n",
            "  Pad token: '<|eot_id|>'\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading tokenizer...\")\n",
        "\n",
        "model_path = model_args.teacher_name_or_path\n",
        "\n",
        "# Use the huggingface_hub library to log in with the token from Colab secrets\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    hf_token = \"\"\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úì Successfully logged in to Hugging Face Hub!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error logging in to Hugging Face Hub: {e}\")\n",
        "    print(\"Please make sure you have added your HF_TOKEN to Colab secrets.\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
        "\n",
        "# Configure padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"‚úì Tokenizer loaded from {model_path}\")\n",
        "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
        "print(f\"  Pad token: '{tokenizer.pad_token}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHwDrRx3Ukhj"
      },
      "source": [
        "## üéØ Step 7: Define Data Formatting Function\n",
        "\n",
        "This function formats our dataset samples into the chat template format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwRtoJKjUkhj",
        "outputId": "89a47ec3-eaa3-494a-dfc5-10e18237f1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Data formatting function defined\n"
          ]
        }
      ],
      "source": [
        "def _format_smoltalk_chat_template(sample, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert dataset sample into chat format.\n",
        "\n",
        "    Args:\n",
        "        sample: Dataset sample with 'query' and 'answer' fields\n",
        "        tokenizer: Tokenizer with chat template\n",
        "\n",
        "    Returns:\n",
        "        Formatted conversation string\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": sample[\"query\"]},\n",
        "        {\"role\": \"assistant\", \"content\": sample[\"answer\"]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "print(\"‚úì Data formatting function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Tc5D4jUkhj"
      },
      "source": [
        "## ü§ñ Step 8: Load Student Model\n",
        "\n",
        "Load the smaller student model that will learn from the teacher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tAV30J0Ukhj",
        "outputId": "caa8ad79-4b8b-4a26-d85c-da24b68fdb2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading student model: meta-llama/Llama-3.2-1B\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Student model loaded!\n",
            "  Parameters: 1,235,814,400 (1.24B)\n",
            "  Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading student model: {model_args.student_name_or_path}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_args.student_name_or_path,\n",
        "    torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "student_params = sum(p.numel() for p in student_model.parameters())\n",
        "print(f\"\\n‚úì Student model loaded!\")\n",
        "print(f\"  Parameters: {student_params:,} ({student_params/1e9:.2f}B)\")\n",
        "print(f\"  Device: {next(student_model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjPB1pnUkhj"
      },
      "source": [
        "## üë®‚Äçüè´ Step 9: Load Teacher Model & Configure Distillation\n",
        "\n",
        "Load the larger teacher model and set up knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "e10833d8c8c2455ca53e0e18ad00be74",
            "dc756811aa424310bd0289758be19c9f",
            "b3ce9e2fb77643ee99d419768090f6b0",
            "bf2af65d8a5b4fa7acecf4b0ba93deac",
            "ef3b4ac0652547e4a106e3aacc0ddfde",
            "3b8d2626b98142f59a10e5e07fc01fbc",
            "636fb2373e0e4973b0612c910a6b2506",
            "aee04f518b6248eb96e397ee34ba3ddc",
            "ad001d23ce924afc9250ea6a544f832a",
            "a05029287a774f398b23c01a1ccfdb96",
            "8cca47091ed8420fab0da9faf7fdd8bf"
          ]
        },
        "id": "vvBvz-GHUkhj",
        "outputId": "9d81229f-c6f8-4139-d394-15cc6c422832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading teacher model: meta-llama/Llama-3.2-3B-Instruct\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e10833d8c8c2455ca53e0e18ad00be74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Teacher model loaded!\n",
            "  Parameters: 1,803,463,680 (1.80B)\n",
            "  Device: cuda:0\n",
            "  Compression ratio: 1.46x\n",
            "\n",
            "Configuring Knowledge Distillation...\n",
            "ModelOpt save/restore enabled for `transformers` library.\n",
            "ModelOpt save/restore enabled for `diffusers` library.\n",
            "ModelOpt save/restore enabled for `peft` library.\n",
            "‚úì Distillation configured!\n",
            "  Loss function: LMLogitsLoss (KL-divergence)\n",
            "  Student will learn from:\n",
            "    1. Ground truth labels\n",
            "    2. Teacher's predictions\n",
            "\n",
            "GPU Memory:\n",
            "  Allocated: 4.51 GB\n",
            "  Reserved: 5.34 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading teacher model: {model_args.teacher_name_or_path}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_args.teacher_name_or_path,\n",
        "    torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float32,\n",
        "    # load_in_4bit=True,  # Add this line for 8-bit quantization\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "print(f\"\\n‚úì Teacher model loaded!\")\n",
        "print(f\"  Parameters: {teacher_params:,} ({teacher_params/1e9:.2f}B)\")\n",
        "print(f\"  Device: {next(teacher_model.parameters()).device}\")\n",
        "print(f\"  Compression ratio: {teacher_params/student_params:.2f}x\")\n",
        "\n",
        "# Configure Knowledge Distillation\n",
        "print(\"\\nConfiguring Knowledge Distillation...\")\n",
        "kd_config = {\n",
        "    \"teacher_model\": teacher_model,\n",
        "    \"criterion\": LMLogitsLoss(),  # KL-divergence on logits\n",
        "}\n",
        "\n",
        "# Enable ModelOpt checkpointing\n",
        "mto.enable_huggingface_checkpointing()\n",
        "\n",
        "# Convert student to distillation model\n",
        "model = mtd.convert(student_model, mode=[(\"kd_loss\", kd_config)])\n",
        "\n",
        "# Fix generation config warnings\n",
        "model.generation_config.temperature = None\n",
        "model.generation_config.top_p = None\n",
        "\n",
        "print(\"‚úì Distillation configured!\")\n",
        "print(\"  Loss function: LMLogitsLoss (KL-divergence)\")\n",
        "print(\"  Student will learn from:\")\n",
        "print(\"    1. Ground truth labels\")\n",
        "print(\"    2. Teacher's predictions\")\n",
        "\n",
        "# Check memory usage\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
        "    print(f\"\\nGPU Memory:\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Reserved: {reserved:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXi1RsfqUkhj"
      },
      "source": [
        "## üèãÔ∏è Step 10: Create Custom Trainer\n",
        "\n",
        "Define a custom trainer that combines supervised fine-tuning with knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UeunJI2Ukhj"
      },
      "outputs": [],
      "source": [
        "class KDSFTTrainer(SFTTrainer, KDTrainer):\n",
        "    \"\"\"\n",
        "    Combined Knowledge Distillation + Supervised Fine-Tuning Trainer.\n",
        "\n",
        "    Inherits from:\n",
        "    - SFTTrainer: Supervised fine-tuning logic\n",
        "    - KDTrainer: Knowledge distillation logic\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3rLdN_LUkhk"
      },
      "source": [
        "## üéì Step 11: Initialize Trainer\n",
        "\n",
        "Set up the trainer with our models, datasets, and configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "20a8ed524adf41a997fad1b6dbc1c23a",
            "0cd6600e5ee24f909f160a947ec58b24",
            "0de72ad40ae54e6a85b4ddb6a47fd189",
            "e9e90312f63e4f3fa7c583e815ebd147",
            "c13e2ddf74394996a6286eddac6018e5",
            "937d8cf2ce5340d1b5af8a9a7cda385a",
            "3a61cc7b2501478aa2f8ceafd1dafcd9",
            "35806c8b67fe44fdbd5fe070d1d986ca",
            "c35d45556e52418b914972bb38d5cdcf",
            "4dc4c950edb54c69bbb89e8488742655",
            "03f2a87c58654239be59f7d320ef2f3b",
            "131b072c2d2646d893d476a4c84eb608",
            "219c2681db4c4b89ab0d85903ce0c794",
            "ceb4fad461414603bbc26980c273fc72",
            "7e35c465320e41e09f17d57dd2b43963",
            "fbe951d3e4fd44a0833bb46791e77320",
            "f146a4bae1c646e3b00559ba043fae59",
            "ff81e28655c2499b90e78ea9291dca21",
            "b4976967f30d44559aea9f2e78b0022b",
            "e39444837a524deeb3a865fba12c3feb",
            "a7fae910858b452fb3acc1df12cfa55e",
            "533433ee1df04ce7bb0b01270c0ce608"
          ]
        },
        "id": "hi4_6fKtUkhk",
        "outputId": "87621852-2700-4066-e58e-69840297a93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing trainer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset (num_proc=4):   0%|          | 0/12800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a8ed524adf41a997fad1b6dbc1c23a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating eval dataset (num_proc=4):   0%|          | 0/1280 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "131b072c2d2646d893d476a4c84eb608"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelOpt save/restore enabled for `transformers` library.\n",
            "ModelOpt save/restore enabled for `diffusers` library.\n",
            "ModelOpt save/restore enabled for `peft` library.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Trainer initialized!\n",
            "  Training steps: 200\n",
            "  Checkpoints: ./llama3.2-1b-distilled\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing trainer...\")\n",
        "\n",
        "trainer = KDSFTTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=dset_train,\n",
        "    eval_dataset=dset_eval,\n",
        "    formatting_func=lambda sample: _format_smoltalk_chat_template(sample, tokenizer),\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"‚úì Trainer initialized!\")\n",
        "print(f\"  Training steps: {training_args.max_steps}\")\n",
        "print(f\"  Checkpoints: {training_args.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUDqJLO1Ukhk"
      },
      "source": [
        "## üöÄ Step 12: Start Training!\n",
        "\n",
        "Now we train the student model with knowledge distillation.\n",
        "\n",
        "**This will take some time!** Monitor the loss values:\n",
        "- **loss**: Combined loss (should decrease)\n",
        "- Lower loss = better learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f3C5TPe9Ukhk",
        "outputId": "0d18bb2b-5a7f-49c6-951e-61de7b8e81ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STARTING TRAINING\n",
            "================================================================================\n",
            "The student is now learning from the teacher...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 14:45, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.341300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.237200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.216800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.085700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.874800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.106200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.941300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.824600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.738200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.834000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.655700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.874300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.848900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.677600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.758500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.908800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.772700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.785900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.770400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.778500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.675500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.732200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.791500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.656100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.847500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.780400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.745100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.722700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.822000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.971200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage at training step 1, device=0: memory (MB) | allocated:  9.35e+03 | max_allocated:  1.41e+04 | reserved:  1.42e+04 | max_reserved:  1.42e+04\n",
            "Saved ModelOpt state to ./llama3.2-1b-distilled/checkpoint-100/modelopt_state.pth\n",
            "Saved ModelOpt state to ./llama3.2-1b-distilled/checkpoint-200/modelopt_state.pth\n",
            "\n",
            "‚úì Training completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "print(\"The student is now learning from the teacher...\\n\")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úì Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-dIoIS1Ukhk"
      },
      "source": [
        "## üìà Step 13: Evaluate the Model\n",
        "\n",
        "Evaluate the trained student model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "Zx4sMUu9Ukhk",
        "outputId": "a8beb608-3b5e-4917-9c1f-8f24cddee436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RUNNING EVALUATION\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1280' max='1280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1280/1280 35:15]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/modelopt/torch/distill/distillation_model.py:312: UserWarning: Teacher's Module `LlamaForCausalLM` already has an intermediate output stored. This is expected when `DistillationModel.compute_kd_loss` is not called in eval mode.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "  eval_loss: 2.0978\n",
            "  eval_runtime: 2116.9910\n",
            "  eval_samples_per_second: 0.6050\n",
            "  eval_steps_per_second: 0.6050\n",
            "  eval_entropy: 1.8538\n",
            "  eval_num_tokens: 70365.0000\n",
            "  eval_mean_token_accuracy: 0.5848\n",
            "  epoch: 0.0156\n",
            "\n",
            "‚úì Evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RUNNING EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n‚úì Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Cvhcg-Ukhk"
      },
      "source": [
        "## üíæ Step 14: Save the Distilled Model\n",
        "\n",
        "Save the trained student model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qau2rKJUkhk",
        "outputId": "ea9827fd-cf52-4710-a5b7-b6b15aff1507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SAVING MODEL\n",
            "================================================================================\n",
            "Saved ModelOpt state to ./llama3.2-1b-distilled/modelopt_state.pth\n",
            "‚úì Model saved to: ./llama3.2-1b-distilled\n",
            "\n",
            "You can now load the model with:\n",
            "  model = AutoModelForCausalLM.from_pretrained('./llama3.2-1b-distilled')\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save training state\n",
        "trainer.save_state()\n",
        "\n",
        "# Save the student model (removes distillation wrapper)\n",
        "trainer.save_model(training_args.output_dir, export_student=True)\n",
        "\n",
        "print(f\"‚úì Model saved to: {training_args.output_dir}\")\n",
        "print(\"\\nYou can now load the model with:\")\n",
        "print(f\"  model = AutoModelForCausalLM.from_pretrained('{training_args.output_dir}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr9aKXv0Ukhk"
      },
      "source": [
        "## üéâ Step 15: Test the Distilled Model (Optional)\n",
        "\n",
        "Let's try generating some text with our newly trained model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXFsbiAFUkhk",
        "outputId": "77dc0abb-ee61-431f-cc02-f86e813a8b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the distilled model...\n",
            "\n",
            "Restored ModelOpt state from ./llama3.2-1b-distilled/modelopt_state.pth\n",
            "Generating response...\n",
            "\n",
            "Model Response:\n",
            "================================================================================\n",
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 27 Oct 2025\n",
            "\n",
            "user\n",
            "\n",
            "What is knowledge distillation?assistant\n",
            "\n",
            "Knowledge distillation is a process of taking raw data and transforming it into a distilled, distilled, distilled version of the original information. It's a method of creating a concise, focused, and accurate summary of a larger body of knowledge. The goal of knowledge distillation is to extract the most valuable lessons from the original data, which can be used to improve decision-making, problem-solving, and overall understanding.\n",
            "\n",
            "Here's a step-by-step guide on how knowledge distillation works:\n",
            "\n",
            "1. **Data collection:** This is where the raw, original data is collected. This could be from a survey, a report, or a dataset.\n",
            "2. **Analysis:** The data is analyzed to identify the most important insights, patterns, and relationships. This is where\n",
            "================================================================================\n",
            "\n",
            "‚úì Inference test complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing the distilled model...\\n\")\n",
        "\n",
        "# Export the student model for inference\n",
        "# inference_model = mtd.export(model) # This line caused the error\n",
        "\n",
        "# Load the saved student model directly from the output directory\n",
        "inference_model = AutoModelForCausalLM.from_pretrained(training_args.output_dir)\n",
        "\n",
        "\n",
        "# Prepare a test prompt\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is knowledge distillation?\"}\n",
        "]\n",
        "test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(inference_model.device)\n",
        "\n",
        "# Generate\n",
        "print(\"Generating response...\\n\")\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Model Response:\")\n",
        "print(\"=\"*80)\n",
        "print(response)\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚úì Inference test complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAKgcOMtUkhk"
      },
      "source": [
        "## üéä Congratulations!\n",
        "\n",
        "You've successfully completed knowledge distillation! üéâ\n",
        "\n",
        "### What you've accomplished:\n",
        "‚úÖ Loaded a large teacher model (3B parameters)  \n",
        "‚úÖ Loaded a small student model (1B parameters)  \n",
        "‚úÖ Configured knowledge distillation with TensorRT Model Optimizer  \n",
        "‚úÖ Trained the student to learn from the teacher  \n",
        "‚úÖ Saved a smaller, faster model with the teacher's knowledge  \n",
        "\n",
        "### Next Steps:\n",
        "- **Fine-tune further**: Increase `max_steps` for better results\n",
        "- **Try different models**: Change teacher/student in the configuration\n",
        "- **Use your own data**: Replace the dataset with your own\n",
        "- **Deploy the model**: Use the saved model for inference\n",
        "\n",
        "### Resources:\n",
        "- [TensorRT Model Optimizer Docs](https://nvidia.github.io/TensorRT-Model-Optimizer/)\n",
        "- [HuggingFace Transformers](https://huggingface.co/docs/transformers)\n",
        "- [TRL Library](https://huggingface.co/docs/trl)\n",
        "\n",
        "---\n",
        "**Happy distilling! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e10833d8c8c2455ca53e0e18ad00be74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc756811aa424310bd0289758be19c9f",
              "IPY_MODEL_b3ce9e2fb77643ee99d419768090f6b0",
              "IPY_MODEL_bf2af65d8a5b4fa7acecf4b0ba93deac"
            ],
            "layout": "IPY_MODEL_ef3b4ac0652547e4a106e3aacc0ddfde"
          }
        },
        "dc756811aa424310bd0289758be19c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8d2626b98142f59a10e5e07fc01fbc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_636fb2373e0e4973b0612c910a6b2506",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "b3ce9e2fb77643ee99d419768090f6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aee04f518b6248eb96e397ee34ba3ddc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad001d23ce924afc9250ea6a544f832a",
            "value": 2
          }
        },
        "bf2af65d8a5b4fa7acecf4b0ba93deac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05029287a774f398b23c01a1ccfdb96",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8cca47091ed8420fab0da9faf7fdd8bf",
            "value": "‚Äá2/2‚Äá[00:38&lt;00:00,‚Äá17.56s/it]"
          }
        },
        "ef3b4ac0652547e4a106e3aacc0ddfde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b8d2626b98142f59a10e5e07fc01fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636fb2373e0e4973b0612c910a6b2506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aee04f518b6248eb96e397ee34ba3ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad001d23ce924afc9250ea6a544f832a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a05029287a774f398b23c01a1ccfdb96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cca47091ed8420fab0da9faf7fdd8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20a8ed524adf41a997fad1b6dbc1c23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cd6600e5ee24f909f160a947ec58b24",
              "IPY_MODEL_0de72ad40ae54e6a85b4ddb6a47fd189",
              "IPY_MODEL_e9e90312f63e4f3fa7c583e815ebd147"
            ],
            "layout": "IPY_MODEL_c13e2ddf74394996a6286eddac6018e5"
          }
        },
        "0cd6600e5ee24f909f160a947ec58b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_937d8cf2ce5340d1b5af8a9a7cda385a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3a61cc7b2501478aa2f8ceafd1dafcd9",
            "value": "Truncating‚Äátrain‚Äádataset‚Äá(num_proc=4):‚Äá100%"
          }
        },
        "0de72ad40ae54e6a85b4ddb6a47fd189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35806c8b67fe44fdbd5fe070d1d986ca",
            "max": 12800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c35d45556e52418b914972bb38d5cdcf",
            "value": 12800
          }
        },
        "e9e90312f63e4f3fa7c583e815ebd147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc4c950edb54c69bbb89e8488742655",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_03f2a87c58654239be59f7d320ef2f3b",
            "value": "‚Äá12800/12800‚Äá[00:00&lt;00:00,‚Äá20963.42‚Äáexamples/s]"
          }
        },
        "c13e2ddf74394996a6286eddac6018e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937d8cf2ce5340d1b5af8a9a7cda385a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a61cc7b2501478aa2f8ceafd1dafcd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35806c8b67fe44fdbd5fe070d1d986ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35d45556e52418b914972bb38d5cdcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dc4c950edb54c69bbb89e8488742655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f2a87c58654239be59f7d320ef2f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "131b072c2d2646d893d476a4c84eb608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_219c2681db4c4b89ab0d85903ce0c794",
              "IPY_MODEL_ceb4fad461414603bbc26980c273fc72",
              "IPY_MODEL_7e35c465320e41e09f17d57dd2b43963"
            ],
            "layout": "IPY_MODEL_fbe951d3e4fd44a0833bb46791e77320"
          }
        },
        "219c2681db4c4b89ab0d85903ce0c794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f146a4bae1c646e3b00559ba043fae59",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ff81e28655c2499b90e78ea9291dca21",
            "value": "Truncating‚Äáeval‚Äádataset‚Äá(num_proc=4):‚Äá100%"
          }
        },
        "ceb4fad461414603bbc26980c273fc72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4976967f30d44559aea9f2e78b0022b",
            "max": 1280,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e39444837a524deeb3a865fba12c3feb",
            "value": 1280
          }
        },
        "7e35c465320e41e09f17d57dd2b43963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7fae910858b452fb3acc1df12cfa55e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_533433ee1df04ce7bb0b01270c0ce608",
            "value": "‚Äá1280/1280‚Äá[00:00&lt;00:00,‚Äá1015.88‚Äáexamples/s]"
          }
        },
        "fbe951d3e4fd44a0833bb46791e77320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f146a4bae1c646e3b00559ba043fae59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff81e28655c2499b90e78ea9291dca21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4976967f30d44559aea9f2e78b0022b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e39444837a524deeb3a865fba12c3feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7fae910858b452fb3acc1df12cfa55e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533433ee1df04ce7bb0b01270c0ce608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}