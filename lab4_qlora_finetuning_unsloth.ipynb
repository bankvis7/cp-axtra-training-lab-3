{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# การ Fine-tuning ด้วย QLoRA โดยใช้ Unsloth\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a>\n",
        "</div>\n",
        "\n",
        "## วัตถุประสงค์การเรียนรู้\n",
        "\n",
        "ใน lab นี้ จะได้:\n",
        "1. เข้าใจว่า QLoRA (Quantized Low-Rank Adaptation) คืออะไรและทำงานอย่างไร\n",
        "2. เตรียมและจัดรูปแบบชุดข้อมูลสำหรับการ fine-tuning แบบ instruction\n",
        "3. กำหนดค่าพารามิเตอร์ LoRA (rank, alpha, dropout, target modules)\n",
        "4. Fine-tune โมเดล Gemma 2 2B บนชุดข้อมูล instruction-following\n",
        "5. เปรียบเทียบประสิทธิภาพระหว่างโมเดลพื้นฐานกับโมเดลที่ fine-tune แล้ว\n",
        "6. วิเคราะห์ประสิทธิภาพด้านหน่วยความจำของ QLoRA เทียบกับการ fine-tuning แบบเต็ม\n",
        "7. บันทึกและส่งออกโมเดลที่ fine-tune แล้วเพื่อการใช้งาน"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## QLoRA คืออะไร?\n",
        "\n",
        "**QLoRA (Quantized Low-Rank Adaptation)** เป็นการรวมเทคนิค:\n",
        "\n",
        "1. **Quantization**: ลดน้ำหนักของโมเดลเหลือ 4-bit precision เพื่อประหยัดหน่วยความจำ\n",
        "2. **LoRA**: ฝึก adapter layers ขนาดเล็กแทนการฝึกทั้งโมเดล\n",
        "\n",
        "### ทำไมต้องใช้ QLoRA?\n",
        "\n",
        "- **ประหยัดหน่วยความจำ**: Fine-tune โมเดลขนาดใหญ่บน GPU สำหรับผู้บริโภคทั่วไป\n",
        "- **การ training ที่รวดเร็ว**: อัพเดทเพียง 1-2% ของพารามิเตอร์โมเดล\n",
        "- **คุณภาพสูง**: ได้ประสิทธิภาพใกล้เคียงกับการ fine-tuning แบบเต็ม\n",
        "- **พกพาได้**: LoRA adapters มีขนาดเล็ก (MBs แทนที่จะเป็น GBs) และสามารถแชร์ได้\n",
        "\n",
        "### LoRA ทำงานอย่างไร\n",
        "\n",
        "แทนที่จะอัพเดทน้ำหนักทั้งหมด `W`, LoRA จะเพิ่มเมทริกซ์ที่ฝึกได้:\n",
        "```\n",
        "W_new = W_frozen + (LoRA_A × LoRA_B)\n",
        "```\n",
        "โดยที่:\n",
        "- `W_frozen`: น้ำหนักต้นฉบับที่ถูกตรึง (quantized เป็น 4-bit)\n",
        "- `LoRA_A, LoRA_B`: เมทริกซ์ขนาดเล็กที่ฝึกได้ (rank r << hidden_dim)\n",
        "\n",
        "สำหรับการรัน lab นี้ ให้กด \"*Runtime*\" และกด \"*Run all*\" บน **free** Tesla T4 Google Colab instance!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## ส่วนที่ 1: การติดตั้ง\n",
        "\n",
        "ก่อนอื่น เรามาติดตั้ง libraries ที่จำเป็น เราจะติดตั้ง Unsloth ซึ่งให้การสนับสนุน QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## ส่วนที่ 2: โหลดโมเดลพื้นฐานด้วย 4-bit Quantization\n",
        "\n",
        "เราจะโหลด Gemma 2 2B ใน 4-bit precision นี่คือรากฐานของ QLoRA - โมเดลจะถูกตรึงไว้ที่ 4-bit ในขณะที่เราฝึก LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING BASE MODEL WITH 4-BIT QUANTIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-2b-it\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,  # QLoRA uses 4-bit quantization\n",
        "    dtype = None,  # Auto-detect dtype\n",
        ")\n",
        "\n",
        "# Track initial memory\n",
        "base_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\nBase model loaded: {base_memory} GB\")\n",
        "print(f\"Model dtype: {model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## ส่วนที่ 3: กำหนดค่า LoRA Adapters\n",
        "\n",
        "ตอนนี้เราจะเพิ่ม LoRA adapters ให้กับโมเดล สิ่งเหล่านี้คือพารามิเตอร์เพียงอย่างเดียวที่จะถูกฝึก\n",
        "\n",
        "### พารามิเตอร์ LoRA หลัก:\n",
        "\n",
        "- **r (rank)**: มิติของเมทริกซ์ LoRA ยิ่งสูง = ความจุมากขึ้นแต่ใช้หน่วยความจำมากขึ้น\n",
        "  - ค่าทั่วไป: 8, 16, 32, 64\n",
        "  - เราจะใช้ 16 เพื่อความสมดุลที่ดี\n",
        "\n",
        "- **lora_alpha**: ตัวคูณสำหรับการอัพเดท LoRA มักจะตั้งเป็น `r` หรือ `2*r`\n",
        "  - ควบคุมขนาดของการอัพเดท adapter\n",
        "  - เราจะใช้ 16 (เท่ากับ r)\n",
        "\n",
        "- **lora_dropout**: ความน่าจะเป็นของ dropout สำหรับเลเยอร์ LoRA\n",
        "  - ป้องกัน overfitting บนชุดข้อมูลขนาดเล็ก\n",
        "  - เราจะใช้ 0 (Unsloth แนะนำค่านี้)\n",
        "\n",
        "- **target_modules**: เลเยอร์ไหนที่จะเพิ่ม LoRA adapters\n",
        "  - ทั่วไป: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] สำหรับ attention\n",
        "  - สามารถรวม [\"gate_proj\", \"up_proj\", \"down_proj\"] สำหรับ FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ADDING LoRA ADAPTERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # LoRA rank\n",
        "    lora_alpha = 16,  # LoRA scaling\n",
        "    lora_dropout = 0,  # No dropout (Unsloth optimized)\n",
        "    \n",
        "    # Target modules: attention layers\n",
        "    target_modules = [\n",
        "        \"q_proj\",  # Query projection\n",
        "        \"k_proj\",  # Key projection\n",
        "        \"v_proj\",  # Value projection\n",
        "        \"o_proj\",  # Output projection\n",
        "        \"gate_proj\",  # FFN gate\n",
        "        \"up_proj\",  # FFN up\n",
        "        \"down_proj\",  # FFN down\n",
        "    ],\n",
        "    \n",
        "    bias = \"none\",  # Don't train bias terms\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# Check trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_percent = 100 * trainable_params / all_params\n",
        "\n",
        "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
        "print(f\"Total parameters: {all_params:,}\")\n",
        "print(f\"Percentage trainable: {trainable_percent:.2f}%\")\n",
        "\n",
        "# Memory after adding LoRA\n",
        "lora_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\nMemory with LoRA adapters: {lora_memory} GB\")\n",
        "print(f\"Additional memory for LoRA: {lora_memory - base_memory:.3f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "## ส่วนที่ 4: เตรียมชุดข้อมูล\n",
        "\n",
        "เราจะใช้ชุดข้อมูล **Alpaca** - คอลเลกชันของตัวอย่าง instruction-following 52K รายการ\n",
        "\n",
        "### รูปแบบชุดข้อมูล\n",
        "\n",
        "แต่ละตัวอย่างมี:\n",
        "- **instruction**: คำอธิบายงาน\n",
        "- **input**: บริบทหรืออินพุตเสริม (อาจจะว่าง)\n",
        "- **output**: ผลลัพธ์ที่ต้องการ\n",
        "\n",
        "เราจะจัดรูปแบบนี้ให้เป็น chat template ของ Gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Load dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING ALPACA DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "print(f\"\\nDataset size: {len(dataset):,} examples\")\n",
        "print(\"\\nExample from dataset:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Instruction: {dataset[0]['instruction']}\")\n",
        "print(f\"Input: {dataset[0]['input']}\")\n",
        "print(f\"Output: {dataset[0]['output']}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "### ตั้งค่า Chat Template\n",
        "\n",
        "เราจะกำหนดค่า chat template ของ Gemma เพื่อจัดรูปแบบข้อมูลของเราอย่างถูกต้อง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma\",  # Use Gemma format\n",
        ")\n",
        "\n",
        "print(\"Chat template configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "### จัดรูปแบบชุดข้อมูล\n",
        "\n",
        "แปลงรูปแบบ Alpaca เป็นรูปแบบ chat ของ Gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_alpaca_to_chat(example):\n",
        "    \"\"\"Convert Alpaca format to chat format\"\"\"\n",
        "    \n",
        "    # Combine instruction and input\n",
        "    if example['input'].strip():\n",
        "        user_message = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
        "    else:\n",
        "        user_message = example['instruction']\n",
        "    \n",
        "    # Create chat messages\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "        {\"role\": \"assistant\", \"content\": example['output']},\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = False,\n",
        "    )\n",
        "    \n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"\\nFormatting dataset...\")\n",
        "dataset = dataset.map(format_alpaca_to_chat)\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\nFormatted example:\")\n",
        "print(\"=\"*60)\n",
        "print(dataset[0]['text'][:500] + \"...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "### สร้างชุดข้อมูล Train/Test\n",
        "\n",
        "เราจะใช้ 1000 ตัวอย่างสำหรับการ training (เพื่อให้การ training รวดเร็ว) และ 100 ตัวอย่างสำหรับการทดสอบ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For this lab, we'll use a subset for faster training\n",
        "train_dataset = dataset.select(range(1000))  # First 1000 examples\n",
        "test_dataset = dataset.select(range(1000, 1100))  # Next 100 examples\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset):,}\")\n",
        "print(f\"Test examples: {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "## ส่วนที่ 5: ทดสอบโมเดลพื้นฐาน (ก่อนการ Fine-tuning)\n",
        "\n",
        "มาทดสอบโมเดลพื้นฐานด้วย prompts สองสามตัวเพื่อสร้างเกณฑ์อ้างอิง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, tokenizer, prompt, max_tokens=200):\n",
        "    \"\"\"Helper function to test model with a prompt\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt = True,\n",
        "        tokenize = True,\n",
        "        return_tensors = \"pt\",\n",
        "        return_dict = True,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = max_tokens,\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.9,\n",
        "        do_sample = True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Write a haiku about artificial intelligence.\",\n",
        "    \"Explain the concept of recursion in programming in simple terms.\",\n",
        "    \"What are three tips for staying productive while working from home?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASE MODEL RESPONSES (BEFORE FINE-TUNING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_responses = []\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[Test {i}] Prompt: {prompt}\")\n",
        "    print(\"-\"*60)\n",
        "    response = test_model(model, tokenizer, prompt)\n",
        "    base_responses.append(response)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {},
      "source": [
        "## ส่วนที่ 6: กำหนดค่าการ Training\n",
        "\n",
        "ตอนนี้เราจะตั้งค่าการ training โดยใช้ Library TRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFIGURING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    \n",
        "    args = TrainingArguments(\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        learning_rate = 2e-4,\n",
        "        warmup_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        logging_steps = 10,\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 50,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        save_strategy = \"epoch\",\n",
        "        save_total_limit = 1,\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",  # Disable wandb, tensorboard, and all logging\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\nTraining configured! Ready to train.\")\n",
        "print(f\"  - Training examples: {len(train_dataset):,}\")\n",
        "print(f\"  - Effective batch size: 8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-20",
      "metadata": {},
      "source": [
        "## ส่วนที่ 7: การ train โมเดล"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "peak_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Peak GPU memory: {peak_memory} GB\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "## ส่วนที่ 8: ทดสอบโมเดลที่ Fine-tune แล้ว\n",
        "\n",
        "มาทดสอบโมเดลที่ fine-tune แล้วด้วย prompts เดิมและเปรียบเทียบกัน"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINE-TUNED MODEL RESPONSES (AFTER FINE-TUNING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "finetuned_responses = []\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[Test {i}] Prompt: {prompt}\")\n",
        "    print(\"-\"*60)\n",
        "    response = test_model(model, tokenizer, prompt)\n",
        "    finetuned_responses.append(response)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {},
      "source": [
        "### การเปรียบเทียบแบบเคียงข้าง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEFORE vs AFTER COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\n[Test {i+1}] {prompt}\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"BEFORE: {base_responses[i]}\")\n",
        "    print(f\"\\nAFTER:  {finetuned_responses[i]}\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {},
      "source": [
        "## ส่วนที่ 9: การวิเคราะห์การใช้หน่วยความจำ\n",
        "\n",
        "มาวิเคราะห์ว่า QLoRA ประหยัดหน่วยความจำได้มากแค่ไหนเมื่อเทียบกับการ fine-tuning แบบเต็ม"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "methods = ['QLoRA\\n(This Lab)', 'Full Fine-tuning\\n(BF16)', 'Full Model\\n(BF16)']\n",
        "memory_usage = [peak_memory, 16, 4]\n",
        "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(methods, memory_usage, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "\n",
        "for bar, mem in zip(bars, memory_usage):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{mem:.1f} GB',\n",
        "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('GPU Memory Usage (GB)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Memory Efficiency: QLoRA vs Full Fine-tuning\\nGemma-2 2B Model', \n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "savings = ((16 - peak_memory) / 16) * 100\n",
        "textstr = f'QLoRA Memory Savings:\\n~{savings:.0f}% vs Full Fine-tuning'\n",
        "props = dict(boxstyle='round', facecolor='lightgreen', alpha=0.8)\n",
        "ax.text(0.98, 0.97, textstr, transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', horizontalalignment='right', bbox=props)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nQLoRA used only ~{(peak_memory/16)*100:.0f}% of the memory required for full fine-tuning!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
