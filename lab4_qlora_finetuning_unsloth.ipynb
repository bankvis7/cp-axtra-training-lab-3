{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d12e366b",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/bankvis7/cp-axtra-training-lab-3/blob/main/lab4_qlora_finetuning_unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Lab การ Fine-tuning ด้วย QLoRA โดยใช้ Unsloth\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a>\n",
        "</div>\n",
        "\n",
        "เนื้อหา:\n",
        "\n",
        "1. การเตรียมและจัดรูปแบบชุดข้อมูลสำหรับการ fine-tuning แบบ instruction\n",
        "2. การ Fine-tune โมเดล Gemma 2 2B บนชุดข้อมูล instruction-following\n",
        "3. กำหนดค่าพารามิเตอร์ LoRA (rank, alpha, dropout, target modules)\n",
        "4. การวิเคราะห์และเปรียบเทียบประสิทธิภาพ base model กับโมเดลที่ fine-tune แล้ว\n",
        "5. การบันทึกโมเดลที่ fine-tune แล้วเพื่อการ Deployment\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## QLoRA (Quantized Low-Rank Adaptation)\n",
        "\n",
        "QLoRA เป็นการรวมเทคนิค:\n",
        "\n",
        "1. **Quantization** ของ base model ด้วย NF4 เพื่อประหยัดหน่วยความจำ\n",
        "2. **LoRA** สร้าง Low-Rank Adapter แทนการสร้างเมทริกซ์ขนาดใหญ่\n",
        "\n",
        "### LoRA ทำงานอย่างไร\n",
        "\n",
        "เราจะไม่แก้ weight ของ base model แต่จะเพิ่ม adapter เพื่อปรับแก้ผลลัพธ์ให้ถูกต้องแทน:\n",
        "\n",
        "$$W_{\\text{new}} = W_{\\text{frozen}} + \\frac{\\alpha}{r} \\; W_A W_B $$\n",
        "\n",
        "โดยที่:\n",
        "- $W_{\\text{new}}$ คือ น้ำหนักของ base model ที่ถูก quantized เป็น 4-bit\n",
        "- $W_A, W_B$ คือ เมทริกซ์ของ adapter\n",
        "- $r$ คือ rank ของ adapter (rank r << hidden_dim)\n",
        "- $\\alpha$ คือ scaling factor เพื่อควบคุมผลกระทบของ adapter\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75653bd",
      "metadata": {},
      "source": [
        "## Hugging Face Token\n",
        "\n",
        "ในการเข้าถึงโมเดล Gemma จะต้องยืนยันตัวตนกับ Hugging Face โดยใช้ read access token จาก [Hugging Face](https://huggingface.co/settings/tokens) และเพิ่มลงใน Colab secrets manager ให้ตั้งชื่อ secret ว่า `HF_TOKEN`\n",
        "\n",
        "หรือ Run Cell ต่อไปนี้เพื่อ Login ผ่าน GUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd0656f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c9f332",
      "metadata": {},
      "source": [
        "* * *\n",
        "\n",
        "## T4 GPU Runtime\n",
        "\n",
        "แลปนี้จำเป็นต้องใช้ GPU ในการประมวลผล ให้เปิดใช้งาน T4 GPU runtime โดยไปที่ \"**Runtime/Connect (มุมบนขวา)**\" -> \"**Change runtime type**\" และเลือก \"**T4 GPU**\" ภายใต้ \"**Hardware accelerator**\"\n",
        "\n",
        "ตรวจสอบการเชื่อมต่อกับ GPU ด้วยคำสั่งต่อไปนี้:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95536881",
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "* * *\n",
        "## ส่วนที่ 1: การติดตั้ง Unsloth Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !uv pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !uv pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !uv pip install --no-deps unsloth\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## ส่วนที่ 2: โหลด Base Model แบบ 4-bit Quantization\n",
        "\n",
        "ทำการโหลด Gemma2 2B แบบ 4-bit precision เพื่อเป็น Base Model ของ QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab6f950b",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"unsloth/gemma-2-2b-it\"\n",
        "MAX_SEQ_LENGTH = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING BASE MODEL WITH 4-BIT QUANTIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    load_in_4bit = True,  # QLoRA uses 4-bit quantization\n",
        "    dtype = None,  # Auto-detect dtype\n",
        ")\n",
        "\n",
        "# Track initial memory\n",
        "base_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\nBase model loaded: {base_memory} GB\")\n",
        "print(f\"Model dtype: {model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## ส่วนที่ 3: กำหนดค่า LoRA Adapters\n",
        "\n",
        "ทำการเพิ่ม LoRA adapters ให้กับโมเดล ส่วนนี้คือพารามิเตอร์เพียงส่วนเดียวที่จะถูก fine-tune\n",
        "\n",
        "พารามิเตอร์ LoRA ที่สำคัญ\n",
        "- r (rank)\n",
        "- lora_alpha\n",
        "- lora_dropout\n",
        "- target_modules\n",
        "\n",
        "รายละเอียดเกี่ยวกับพารามิเตอร์เหล่านี้สามารถดูได้ที่ [Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#hyperparameters-and-recommendations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ADDING LoRA ADAPTERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # LoRA rank\n",
        "    lora_alpha = 16,  # LoRA scaling\n",
        "    lora_dropout = 0,  # No dropout (Unsloth optimized)\n",
        "    \n",
        "    # Target modules: attention layers\n",
        "    target_modules = [\n",
        "        \"q_proj\",  # Query projection\n",
        "        \"k_proj\",  # Key projection\n",
        "        \"v_proj\",  # Value projection\n",
        "        \"o_proj\",  # Output projection\n",
        "        \"gate_proj\",  # FFN gate\n",
        "        \"up_proj\",  # FFN up\n",
        "        \"down_proj\",  # FFN down\n",
        "    ],\n",
        "    \n",
        "    bias = \"none\",  # Don't train bias terms\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# Check trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_percent = 100 * trainable_params / all_params\n",
        "\n",
        "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
        "print(f\"Total parameters: {all_params:,}\")\n",
        "print(f\"Percentage trainable: {trainable_percent:.2f}%\")\n",
        "\n",
        "# Memory after adding LoRA\n",
        "lora_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\nMemory with LoRA adapters: {lora_memory} GB\")\n",
        "print(f\"Additional memory for LoRA: {lora_memory - base_memory:.3f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "## ส่วนที่ 4: เตรียมชุดข้อมูล\n",
        "\n",
        "ในแลปนี้ เราจะใช้ชุดข้อมูล [**Alpaca**](https://huggingface.co/datasets/yahma/alpaca-cleaned) เป็นตัวอย่างแบบ instruction-following จำนวน 52K รายการ\n",
        "\n",
        "โดยรูปแบบของแต่ละรายการ ประกอบด้วย:\n",
        "- **instruction**: คำสั่งหรือพรอมต์\n",
        "- **input**: บริบทหรืออินพุตเสริม (ถ้ามี)\n",
        "- **output**: ผลลัพธ์ที่ต้องการ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Load dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING ALPACA DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "print(f\"\\nDataset size: {len(dataset):,} examples\")\n",
        "print(\"\\nExample from dataset:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Instruction: {dataset[0]['instruction']}\")\n",
        "print(f\"Input: {dataset[0]['input']}\")\n",
        "print(f\"Output: {dataset[0]['output']}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "### ตั้งค่า Chat Template\n",
        "\n",
        "กำหนด chat template ของ Gemma เพื่อใช้จัดรูปแบบข้อมูลให้ตรงกับที่โมเดลต้องการ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma\",  # Use Gemma format\n",
        ")\n",
        "\n",
        "print(\"Chat template configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "### จัดรูปแบบชุดข้อมูล\n",
        "\n",
        "แปลงรูปแบบ Alpaca เป็นรูปแบบ chat ของ Gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_alpaca_to_chat(example):\n",
        "    \"\"\"Convert Alpaca format to chat format\"\"\"\n",
        "    \n",
        "    # Combine instruction and input\n",
        "    if example['input'].strip():\n",
        "        user_message = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
        "    else:\n",
        "        user_message = example['instruction']\n",
        "    \n",
        "    # Create chat messages\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "        {\"role\": \"assistant\", \"content\": example['output']},\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = False,\n",
        "    )\n",
        "    \n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"\\nFormatting dataset...\")\n",
        "dataset = dataset.map(format_alpaca_to_chat)\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\nFormatted example:\")\n",
        "print(\"=\"*60)\n",
        "print(dataset[0]['text'][:500] + \"...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "### สร้างชุดข้อมูล Train/Test\n",
        "\n",
        "เราจะใช้ 1000 ตัวอย่างสำหรับการ training (เพื่อให้การ training รวดเร็ว) และ 100 ตัวอย่างสำหรับการทดสอบ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For this lab, we'll use a subset for faster training\n",
        "train_dataset = dataset.select(range(1000))  # First 1000 examples\n",
        "test_dataset = dataset.select(range(1000, 1100))  # Next 100 examples\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset):,}\")\n",
        "print(f\"Test examples: {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "## ส่วนที่ 5: ทดสอบ Base Model (ก่อน Fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fc284f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Write a haiku about artificial intelligence.\",\n",
        "    \"Explain the concept of recursion in programming in simple terms.\",\n",
        "    \"What are three tips for staying productive while working from home?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, tokenizer, prompt, max_tokens=200):\n",
        "    \"\"\"Helper function to test model with a prompt\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt = True,\n",
        "        tokenize = True,\n",
        "        return_tensors = \"pt\",\n",
        "        return_dict = True,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = max_tokens,\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.9,\n",
        "        do_sample = True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASE MODEL RESPONSES (BEFORE FINE-TUNING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_responses = []\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[Test {i}] Prompt: {prompt}\")\n",
        "    print(\"-\"*60)\n",
        "    response = test_model(model, tokenizer, prompt)\n",
        "    base_responses.append(response)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {},
      "source": [
        "## ส่วนที่ 6: กำหนดค่าการ Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFIGURING FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    \n",
        "    args = TrainingArguments(\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        learning_rate = 2e-4,\n",
        "        warmup_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        logging_steps = 10,\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 50,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        save_strategy = \"epoch\",\n",
        "        save_total_limit = 1,\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",  # Disable wandb, tensorboard, and all logging\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\nFine-tuning configured! Ready to fine-tune.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-20",
      "metadata": {},
      "source": [
        "## ส่วนที่ 7: การ Fine-tune โมเดล"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "# Fine-tuning!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "peak_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINE-TUNING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Fine-tuning time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Peak GPU memory: {peak_memory} GB\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "## ส่วนที่ 8: ทดสอบโมเดลที่ Fine-tune แล้ว"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINE-TUNED MODEL RESPONSES (AFTER FINE-TUNING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "finetuned_responses = []\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[Test {i}] Prompt: {prompt}\")\n",
        "    print(\"-\"*60)\n",
        "    response = test_model(model, tokenizer, prompt)\n",
        "    finetuned_responses.append(response)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {},
      "source": [
        "### การเปรียบเทียบแบบเคียงข้าง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEFORE vs AFTER COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\n[Test {i+1}] {prompt}\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"BEFORE: {base_responses[i]}\")\n",
        "    print(f\"\\nAFTER:  {finetuned_responses[i]}\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {},
      "source": [
        "## ส่วนที่ 9: การวิเคราะห์การใช้หน่วยความจำ\n",
        "\n",
        "มาวิเคราะห์ว่า QLoRA ประหยัดหน่วยความจำได้มากแค่ไหนเมื่อเทียบกับการ fine-tuning แบบเต็ม"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "methods = ['QLoRA\\n(This Lab)', 'Full Fine-tuning\\n(BF16)', 'Full Model\\n(BF16)']\n",
        "memory_usage = [peak_memory, 16, 4]\n",
        "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(methods, memory_usage, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "\n",
        "for bar, mem in zip(bars, memory_usage):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{mem:.1f} GB',\n",
        "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('GPU Memory Usage (GB)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Memory Efficiency: QLoRA vs Full Fine-tuning\\nGemma-2 2B Model', \n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "savings = ((16 - peak_memory) / 16) * 100\n",
        "textstr = f'QLoRA Memory Savings:\\n~{savings:.0f}% vs Full Fine-tuning'\n",
        "props = dict(boxstyle='round', facecolor='lightgreen', alpha=0.8)\n",
        "ax.text(0.98, 0.97, textstr, transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', horizontalalignment='right', bbox=props)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nQLoRA used only ~{(peak_memory/16)*100:.0f}% of the memory required for full fine-tuning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb38b87c",
      "metadata": {},
      "source": [
        "## ส่วนที่ 10: บันทึกโมเดล\n",
        "\n",
        "บันทึกเฉพาะน้ำหนักของ LoRA Adapters ที่ Fine-tune แล้ว (~10-50 MB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df754012",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING LoRA ADAPTERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.save_pretrained(\"gemma-2-2b-alpaca-lora\")\n",
        "tokenizer.save_pretrained(\"gemma-2-2b-alpaca-lora\")\n",
        "\n",
        "print(\"\\nLoRA adapters saved to: gemma-2-2b-alpaca-lora/\")\n",
        "\n",
        "adapter_path = \"gemma-2-2b-alpaca-lora/adapter_model.safetensors\"\n",
        "if os.path.exists(adapter_path):\n",
        "    size_mb = os.path.getsize(adapter_path) / (1024 * 1024)\n",
        "    print(f\"Adapter size: {size_mb:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbaf5d98",
      "metadata": {},
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
