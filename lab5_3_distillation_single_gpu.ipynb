{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation for LLMs - Single GPU Lab\n",
    "\n",
    "This notebook demonstrates **Knowledge Distillation** using NVIDIA TensorRT Model Optimizer on a single GPU.\n",
    "\n",
    "## What is Knowledge Distillation?\n",
    "\n",
    "Knowledge Distillation is a model compression technique where:\n",
    "- A smaller **student model** learns to mimic a larger **teacher model**\n",
    "- The student learns from both:\n",
    "  1. Ground truth labels (standard supervised learning)\n",
    "  2. Soft predictions from the teacher (knowledge transfer)\n",
    "\n",
    "### In this lab:\n",
    "- **Teacher**: Llama-3.2-3B-Instruct (3.2B parameters)\n",
    "- **Student**: Llama-3.2-1B (1.2B parameters)\n",
    "- **Goal**: Compress the teacher's knowledge into a 2.6Ã— smaller model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Lab Setup\n",
    "\n",
    "### Prerequisites\n",
    "- Single GPU with ~40GB VRAM (e.g., A100, A6000)\n",
    "- Python 3.10+\n",
    "- CUDA 11.8+\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand knowledge distillation process\n",
    "2. Implement dataset preprocessing for LLM training\n",
    "3. Configure and run distributed training\n",
    "4. Evaluate distilled model performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 1: GPU Configuration\n",
    "\n",
    "**IMPORTANT**: Configure which GPU to use BEFORE importing PyTorch!\n",
    "\n",
    "Check available GPUs with `nvidia-smi` or `nvitop` and select a free GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: uv in /home/kpaksaran/.local/lib/python3.10/site-packages (0.9.8)\n",
      "Requirement already satisfied: nvitop in /home/kpaksaran/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /usr/lib/python3/dist-packages (from nvitop) (5.9.0)\n",
      "Requirement already satisfied: nvidia-ml-py<13.581.0a0,>=11.450.51 in /home/kpaksaran/.local/lib/python3.10/site-packages (from nvitop) (13.580.82)\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m90 packages\u001b[0m \u001b[2min 904ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m     0 B/858.14 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 14.88 KiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 30.88 KiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 46.88 KiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 62.88 KiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 78.88 KiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 94.88 KiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 110.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 126.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 142.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 158.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 174.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 190.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 206.88 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 214.12 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 230.12 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 246.12 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 262.12 KiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)-------------------\u001b[0m\u001b[0m 1.94 MiB/858.14 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 3.61 MiB/858.14 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 5.05 MiB/858.14 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 8.00 MiB/858.14 MiB         \u001b[1A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m-\u001b[30m\u001b[2m-----------------------------\u001b[0m\u001b[0m 479.22 KiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 8.00 MiB/858.14 MiB         \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 927.22 KiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 8.00 MiB/858.14 MiB         \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 927.22 KiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 8.02 MiB/858.14 MiB         \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 927.22 KiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 8.03 MiB/858.14 MiB         \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 927.22 KiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 8.54 MiB/858.14 MiB         \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 927.22 KiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 9.85 MiB/858.14 MiB         \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m---------\u001b[30m\u001b[2m---------------------\u001b[0m\u001b[0m 2.55 MiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 10.50 MiB/858.14 MiB        \u001b[2A\n",
      "\u001b[2mtorchvision         \u001b[0m \u001b[32m----------------------------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 7.39 MiB/7.67 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 10.79 MiB/858.14 MiB        \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 10.84 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 11.56 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-------------------\u001b[0m\u001b[0m 11.90 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 12.19 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 12.48 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 12.75 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 13.06 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 13.33 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 13.59 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 13.81 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 14.09 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 14.34 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 14.61 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 14.84 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 15.08 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 15.59 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 16.79 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 17.30 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 17.73 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 18.00 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 18.02 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 19.69 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 24.00 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 29.03 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 34.39 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 38.59 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 39.99 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 40.02 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 40.03 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 40.05 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 40.06 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 44.58 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 49.61 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 54.46 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 59.37 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 64.11 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 69.03 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 74.02 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 77.39 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 81.47 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 86.03 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 86.95 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 87.02 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 87.03 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 87.05 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 87.25 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 91.28 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 95.87 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 95.87 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 96.02 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 96.03 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 96.05 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 96.06 MiB/858.14 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 100.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 105.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 110.17 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 115.52 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 120.81 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 123.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 125.57 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 129.82 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 133.51 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 138.37 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 143.27 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 149.26 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 155.55 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 161.22 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 167.83 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 174.28 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 180.39 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 186.76 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 193.09 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 199.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 205.40 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 211.36 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 216.89 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 223.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 229.26 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 235.81 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 241.84 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 242.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 242.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 242.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 242.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 242.69 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 248.61 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 251.98 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 252.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 252.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 252.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 252.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 254.15 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 260.28 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 266.31 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 272.14 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 278.59 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 284.76 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 291.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 297.32 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 297.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 297.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 298.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 298.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 298.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 302.43 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 308.50 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 314.33 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 318.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 318.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 318.01 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 318.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 318.04 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 318.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 322.64 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 328.80 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 334.59 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------------------\u001b[0m\u001b[0m 340.31 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 343.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 344.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 344.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 344.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 344.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 344.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 347.88 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 347.88 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 348.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 348.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 348.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 348.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 352.69 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 358.40 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 364.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m------------------\u001b[0m\u001b[0m 369.42 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 374.86 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 380.34 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 385.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 389.96 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 390.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 390.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 390.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 390.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 390.19 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 395.90 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 396.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 396.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 396.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 396.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 396.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 399.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 399.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 399.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 399.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)2m-----------------\u001b[0m\u001b[0m 399.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 401.45 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 408.64 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 410.95 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 410.95 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 411.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 411.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 411.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 414.42 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 419.93 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 420.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 420.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 420.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 420.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 420.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[2m----------------\u001b[0m\u001b[0m 425.78 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 431.67 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 434.95 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 435.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 435.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 435.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 435.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 437.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 442.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 442.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 443.01 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 443.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 443.04 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 443.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 446.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 451.09 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[2m---------------\u001b[0m\u001b[0m 457.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 463.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 468.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 474.00 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 474.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 474.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 474.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 474.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 475.37 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)m\u001b[2m--------------\u001b[0m\u001b[0m 481.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 486.51 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 490.86 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 490.86 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 491.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 491.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 491.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 491.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 495.87 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 501.67 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 507.14 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)0m\u001b[2m-------------\u001b[0m\u001b[0m 513.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)30m\u001b[2m------------\u001b[0m\u001b[0m 518.47 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)30m\u001b[2m------------\u001b[0m\u001b[0m 524.20 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)30m\u001b[2m------------\u001b[0m\u001b[0m 529.87 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)30m\u001b[2m------------\u001b[0m\u001b[0m 535.14 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)30m\u001b[2m------------\u001b[0m\u001b[0m 539.78 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[30m\u001b[2m-----------\u001b[0m\u001b[0m 544.82 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[30m\u001b[2m-----------\u001b[0m\u001b[0m 550.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[30m\u001b[2m-----------\u001b[0m\u001b[0m 556.09 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[30m\u001b[2m-----------\u001b[0m\u001b[0m 561.76 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)[30m\u001b[2m-----------\u001b[0m\u001b[0m 567.82 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 573.54 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 579.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 585.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 591.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 596.51 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 602.77 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 608.20 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 613.55 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 619.47 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 625.45 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 631.50 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 637.20 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 642.92 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 648.72 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 654.39 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---\u001b[30m\u001b[2m-------\u001b[0m\u001b[0m 659.83 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---\u001b[30m\u001b[2m-------\u001b[0m\u001b[0m 665.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---\u001b[30m\u001b[2m-------\u001b[0m\u001b[0m 671.06 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---\u001b[30m\u001b[2m-------\u001b[0m\u001b[0m 676.59 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---\u001b[30m\u001b[2m-------\u001b[0m\u001b[0m 681.41 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 687.25 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 693.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 699.19 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 705.56 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 712.58 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-----\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 720.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-----\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 726.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-----\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 734.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-----\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 741.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)------\u001b[30m\u001b[2m----\u001b[0m\u001b[0m 748.40 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)------\u001b[30m\u001b[2m----\u001b[0m\u001b[0m 753.78 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)------\u001b[30m\u001b[2m----\u001b[0m\u001b[0m 759.86 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)------\u001b[30m\u001b[2m----\u001b[0m\u001b[0m 765.50 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)------\u001b[30m\u001b[2m----\u001b[0m\u001b[0m 769.03 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 773.26 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 779.16 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 785.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 791.08 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 797.15 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 803.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 808.65 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 814.45 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 819.87 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 825.29 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 831.14 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 837.02 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 842.17 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 847.59 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 852.05 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 854.19 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 855.26 MiB/858.14 MiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 13.93s\u001b[0m\u001b[0m                                                     \u001b[1A\n",
      "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 9.48s\u001b[0m\u001b[0m\n",
      "\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/5] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 20.49s\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0+cu126\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.1\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 2.57s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m67 packages\u001b[0m \u001b[2min 37ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 3.01s\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# For faster library installation\n",
    "!pip install uv nvitop\n",
    "# Install TensorRT Model Optimizer with HuggingFace support\n",
    "!uv pip install -U nvidia-modelopt[hf]\n",
    "\n",
    "!uv pip uninstall numpy transformers\n",
    "# Install additional dependencies\n",
    "!uv pip install pyarrow 'transformers<5.0' 'trl>=0.23.0' 'numpy<2.0' bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ GPU Configuration:\n",
      "   Selected GPU: 4\n",
      "   CUDA_VISIBLE_DEVICES = 4\n",
      "\n",
      "âš ï¸  After this setting, GPU 4 will appear as 'cuda:0' in PyTorch\n",
      "   This is normal and expected!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "GPU_ID = 4  # Change this to your available GPU (0-7)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_ID)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(f\"ðŸŽ¯ GPU Configuration:\")\n",
    "print(f\"   Selected GPU: {GPU_ID}\")\n",
    "print(f\"   CUDA_VISIBLE_DEVICES = {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "print(f\"\\nâš ï¸  After this setting, GPU {GPU_ID} will appear as 'cuda:0' in PyTorch\")\n",
    "print(f\"   This is normal and expected!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 2: Import Libraries\n",
    "\n",
    "Now that GPU is configured, we can import PyTorch and other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/kpaksaran/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "==> PyTorch version: 2.9.1+cu128\n",
      "==> Transformers version: 4.57.1\n",
      "==> CUDA available: True\n",
      "==> Visible GPU count: 1\n",
      "==> Device 0 name: NVIDIA A100-SXM4-40GB\n",
      "==> Total memory: 39.39 GB\n",
      "\n",
      "âœ… SUCCESS: Using GPU 4 as 'cuda:0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/kpaksaran/.venv/lib/python3.11/site-packages/modelopt/torch/__init__.py:36: UserWarning: transformers version 4.57.1 is incompatible with nvidia-modelopt and may cause issues. Please install recommended version with `pip install nvidia-modelopt[hf]` if working with HF models.\n",
      "  _warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "\n",
    "# TensorRT Model Optimizer imports\n",
    "import modelopt.torch.distill as mtd\n",
    "import modelopt.torch.opt as mto\n",
    "from modelopt.torch.distill.plugins.huggingface import KDTrainer, LMLogitsLoss\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"==> PyTorch version: {torch.__version__}\")\n",
    "print(f\"==> Transformers version: {transformers.__version__}\")\n",
    "print(f\"==> CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"==> Visible GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"==> Device 0 name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"==> Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"\\nâœ… SUCCESS: Using GPU {GPU_ID} as 'cuda:0'\")\n",
    "else:\n",
    "    print(\"âŒ ERROR: CUDA not available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 3: Training Configuration\n",
    "\n",
    "Configure the model paths and training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration:\n",
      "================================================================================\n",
      "  Teacher: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Student: meta-llama/Llama-3.2-1B\n",
      "\n",
      "ðŸ“Š Training Parameters:\n",
      "  Batch size per device:      2\n",
      "  Gradient accumulation:      16\n",
      "  Effective batch size:       32\n",
      "  Learning rate:              2e-05\n",
      "  Max steps:                  3200\n",
      "\n",
      "ðŸ’¾ Output: ./llama3.2-1b-distilled-1gpu\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/kpaksaran/.venv/lib/python3.11/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Model Configuration\"\"\"\n",
    "    # Teacher: Larger model we distill FROM\n",
    "    teacher_name_or_path: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    \n",
    "    # Student: Smaller model we distill TO\n",
    "    student_name_or_path: str = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "\n",
    "# Create model configuration\n",
    "model_args = ModelArguments()\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./llama3.2-1b-distilled-1gpu\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    \n",
    "    # Training duration\n",
    "    max_steps=3200,  # ~2 epochs with our dataset\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    \n",
    "    # Batch size - adjust based on GPU memory\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,  # Effective batch = 2 * 16 = 32\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Mixed precision (faster training, less memory)\n",
    "    bf16=True,\n",
    "    tf32=False,\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=50,\n",
    "    eval_steps=400,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Data processing\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "print(\"âœ… Configuration:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Teacher: {model_args.teacher_name_or_path}\")\n",
    "print(f\"  Student: {model_args.student_name_or_path}\")\n",
    "print(f\"\\nðŸ“Š Training Parameters:\")\n",
    "print(f\"  Batch size per device:      {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation:      {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size:       {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate:              {training_args.learning_rate}\")\n",
    "print(f\"  Max steps:                  {training_args.max_steps}\")\n",
    "print(f\"\\nðŸ’¾ Output: {training_args.output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 4: Load and Prepare Dataset\n",
    "\n",
    "We'll use the **smol-smoltalk-Interaction-SFT** dataset, which contains conversational query-answer pairs.\n",
    "\n",
    "### Dataset Preprocessing\n",
    "The raw dataset has columns: `query`, `answer`, `source`\n",
    "\n",
    "We need to:\n",
    "1. Format into chat template (user/assistant messages)\n",
    "2. Tokenize the text\n",
    "3. Create `input_ids`, `attention_mask`, and `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "âœ… Dataset loaded!\n",
      "  Training samples: 12,800\n",
      "  Evaluation samples: 1,280\n",
      "\n",
      "ðŸ“ Sample data:\n",
      "  Query: What are Data visualization types....\n",
      "  Answer: Data visualization types are diverse and can be categorized based on their purpose, structure, and f...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load the dataset from HuggingFace\n",
    "dset = datasets.load_dataset(\"ReactiveAI/smol-smoltalk-Interaction-SFT\", split=\"train\")\n",
    "\n",
    "# Split into training and evaluation sets\n",
    "dset_splits = dset.train_test_split(train_size=12800, test_size=1280, seed=420)\n",
    "dset_train, dset_eval = dset_splits[\"train\"], dset_splits[\"test\"]\n",
    "\n",
    "print(f\"âœ… Dataset loaded!\")\n",
    "print(f\"  Training samples: {len(dset_train):,}\")\n",
    "print(f\"  Evaluation samples: {len(dset_eval):,}\")\n",
    "print(f\"\\nðŸ“ Sample data:\")\n",
    "print(f\"  Query: {dset_train[0]['query'][:100]}...\")\n",
    "print(f\"  Answer: {dset_train[0]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ Step 5: Load Tokenizer and Preprocess Dataset\n",
    "\n",
    "The tokenizer converts text into tokens that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "âœ… Tokenizer loaded\n",
      "  Vocab size: 128,256\n",
      "  Pad token: '<|eot_id|>'\n",
      "  EOS token: '<|eot_id|>'\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.teacher_name_or_path, use_fast=True)\n",
    "\n",
    "# Configure padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "print(f\"  Pad token: '{tokenizer.pad_token}'\")\n",
    "print(f\"  EOS token: '{tokenizer.eos_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Function\n",
    "\n",
    "This function:\n",
    "1. Converts query/answer into chat format\n",
    "2. Applies the chat template\n",
    "3. Tokenizes with truncation\n",
    "4. Creates labels for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing function defined\n",
      "\n",
      "ðŸ“Š Test output:\n",
      "  Input IDs length: 486\n",
      "  Attention mask length: 486\n",
      "  Labels length: 486\n"
     ]
    }
   ],
   "source": [
    "def format_sample(sample):\n",
    "    \"\"\"\n",
    "    Format and tokenize a dataset sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dict with 'query' and 'answer' keys\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'input_ids', 'attention_mask', and 'labels'\n",
    "    \"\"\"\n",
    "    # Create chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": sample[\"query\"]},\n",
    "        {\"role\": \"assistant\", \"content\": sample[\"answer\"]},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    tokenized = tokenizer(\n",
    "        text, \n",
    "        truncation=True, \n",
    "        max_length=512,  # Limit sequence length\n",
    "        padding=False    # Dynamic padding in data collator\n",
    "    )\n",
    "    \n",
    "    # Create labels (copy of input_ids for language modeling)\n",
    "    tokenized[\"labels\"] = list(tokenized[\"input_ids\"])\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"âœ… Preprocessing function defined\")\n",
    "\n",
    "# Test the function on one sample\n",
    "test_sample = format_sample(dset_train[0])\n",
    "print(f\"\\nðŸ“Š Test output:\")\n",
    "print(f\"  Input IDs length: {len(test_sample['input_ids'])}\")\n",
    "print(f\"  Attention mask length: {len(test_sample['attention_mask'])}\")\n",
    "print(f\"  Labels length: {len(test_sample['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Preprocessing to Dataset\n",
    "\n",
    "This will tokenize all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train set (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12800/12800 [00:04<00:00, 3149.60 examples/s]\n",
      "Tokenizing eval set (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1280/1280 [00:00<00:00, 1480.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Tokenization complete!\n",
      "  Train set columns: ['input_ids', 'attention_mask', 'labels']\n",
      "  Train set features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing datasets...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Apply to training set\n",
    "dset_train = dset_train.map(\n",
    "    format_sample, \n",
    "    remove_columns=dset_train.column_names,  # Remove original columns\n",
    "    num_proc=4,  # Parallel processing\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "# Apply to evaluation set\n",
    "dset_eval = dset_eval.map(\n",
    "    format_sample,\n",
    "    remove_columns=dset_eval.column_names,\n",
    "    num_proc=4,\n",
    "    desc=\"Tokenizing eval set\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Tokenization complete!\")\n",
    "print(f\"  Train set columns: {dset_train.column_names}\")\n",
    "print(f\"  Train set features: {dset_train.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 6: Load Student Model\n",
    "\n",
    "Load the smaller student model that will learn from the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading student model: meta-llama/Llama-3.2-1B\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Student model loaded!\n",
      "  Parameters: 1,235,814,400 (1.24B)\n",
      "  Device: cuda:0\n",
      "  Memory: 2.30 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading student model: {model_args.student_name_or_path}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.student_name_or_path,\n",
    "    torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float32,\n",
    "    device_map={\"\":0}  # Load on first GPU\n",
    ")\n",
    "\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "\n",
    "print(f\"âœ… Student model loaded!\")\n",
    "print(f\"  Parameters: {student_params:,} ({student_params/1e9:.2f}B)\")\n",
    "print(f\"  Device: {next(student_model.parameters()).device}\")\n",
    "print(f\"  Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘¨â€ðŸ« Step 7: Load Teacher Model & Configure Distillation\n",
    "\n",
    "Load the larger teacher model and set up knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading teacher model: meta-llama/Llama-3.2-3B-Instruct\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Teacher model loaded!\n",
      "  Parameters: 3,212,749,824 (3.21B)\n",
      "  Device: cuda:0\n",
      "  Compression ratio: 2.60x\n",
      "  Total GPU memory: 8.29 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading teacher model: {model_args.teacher_name_or_path}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.teacher_name_or_path,\n",
    "    torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float32,\n",
    "    device_map={\"\":0}  # Load on first GPU\n",
    ")\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "\n",
    "print(f\"âœ… Teacher model loaded!\")\n",
    "print(f\"  Parameters: {teacher_params:,} ({teacher_params/1e9:.2f}B)\")\n",
    "print(f\"  Device: {next(teacher_model.parameters()).device}\")\n",
    "print(f\"  Compression ratio: {teacher_params/student_params:.2f}x\")\n",
    "print(f\"  Total GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Knowledge Distillation\n",
    "\n",
    "Set up the distillation loss function and convert the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring knowledge distillation...\n",
      "\n",
      "ModelOpt save/restore enabled for `transformers` library.\n",
      "ModelOpt save/restore enabled for `diffusers` library.\n",
      "ModelOpt save/restore enabled for `peft` library.\n",
      "âœ… Distillation configured!\n",
      "  Loss function: LMLogitsLoss (KL-divergence)\n",
      "  Student will learn from:\n",
      "    1. Ground truth labels (standard loss)\n",
      "    2. Teacher's soft predictions (distillation loss)\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring knowledge distillation...\\n\")\n",
    "\n",
    "# Configure KD loss\n",
    "kd_config = {\n",
    "    \"teacher_model\": teacher_model,\n",
    "    \"criterion\": LMLogitsLoss(),  # KL-divergence on logits\n",
    "}\n",
    "\n",
    "# Enable ModelOpt checkpointing for HuggingFace\n",
    "mto.enable_huggingface_checkpointing()\n",
    "\n",
    "# Convert student to distillation model\n",
    "model = mtd.convert(student_model, mode=[(\"kd_loss\", kd_config)])\n",
    "\n",
    "# Fix generation config warnings\n",
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_p = None\n",
    "\n",
    "print(\"âœ… Distillation configured!\")\n",
    "print(f\"  Loss function: LMLogitsLoss (KL-divergence)\")\n",
    "print(f\"  Student will learn from:\")\n",
    "print(f\"    1. Ground truth labels (standard loss)\")\n",
    "print(f\"    2. Teacher's soft predictions (distillation loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 8: Setup Trainer and Start Training\n",
    "\n",
    "Create the trainer with the appropriate data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up trainer...\n",
      "\n",
      "ModelOpt save/restore enabled for `transformers` library.\n",
      "ModelOpt save/restore enabled for `diffusers` library.\n",
      "ModelOpt save/restore enabled for `peft` library.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trainer ready!\n",
      "\n",
      "ðŸ“Š Training schedule:\n",
      "  Total samples: 12,800\n",
      "  Batch size: 2\n",
      "  Gradient accumulation: 16\n",
      "  Effective batch size: 32\n",
      "  Total steps: 3,200\n",
      "  Estimated time: ~80 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up trainer...\\n\")\n",
    "\n",
    "# Use DataCollatorForSeq2Seq which handles labels properly\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create KD Trainer\n",
    "trainer = KDTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_eval,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready!\")\n",
    "print(f\"\\nðŸ“Š Training schedule:\")\n",
    "print(f\"  Total samples: {len(dset_train):,}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total steps: {training_args.max_steps:,}\")\n",
    "print(f\"  Estimated time: ~{training_args.max_steps * 1.5 / 60:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "This will train for 3,200 steps (~2 epochs).\n",
    "\n",
    "**Note**: Training will take approximately 50-60 minutes on a single A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "\n",
      "â±ï¸  This will take approximately 50-60 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 1:46:27, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>32.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>15.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>7.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.414600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.389800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.920200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.911700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.937700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.925100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage at training step 1, device=0: memory (MB) | allocated:  1.32e+04 | max_allocated:  1.79e+04 | reserved:  1.81e+04 | max_reserved:  1.81e+04\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-500/modelopt_state.pth\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-1000/modelopt_state.pth\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-1500/modelopt_state.pth\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-2000/modelopt_state.pth\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-2500/modelopt_state.pth\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-3000/modelopt_state.pth\n",
      "Saved ModelOpt state to ./llama3.2-1b-distilled-1gpu/checkpoint-3200/modelopt_state.pth\n",
      "\n",
      "================================================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâ±ï¸  This will take approximately 50-60 minutes...\\n\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 9: Save the Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "\n",
      "âš ï¸  Model doesn't have export() method\n",
      "âœ… Using checkpoint-3200 as final model (already saved during training)\n",
      "âœ… Final model available at: ./llama3.2-1b-distilled-1gpu/checkpoint-3200\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n",
      "ðŸ“‚ Available checkpoints:\n",
      "  - checkpoint-1000\n",
      "  - checkpoint-1500\n",
      "  - checkpoint-2000\n",
      "  - checkpoint-2500\n",
      "  - checkpoint-3000\n",
      "  - checkpoint-3200\n",
      "  - checkpoint-500\n",
      "\n",
      "ðŸ’¡ To use the model, load from the latest checkpoint:\n",
      "   AutoModelForCausalLM.from_pretrained('./llama3.2-1b-distilled-1gpu/checkpoint-3200')\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model...\\n\")\n",
    "\n",
    "# Save training state\n",
    "trainer.save_state()\n",
    "\n",
    "# Save the distilled student model\n",
    "# Note: export_student parameter may not work with all ModelOpt versions\n",
    "# If it fails, the model is already saved in checkpoints\n",
    "try:\n",
    "    # Try to export student model\n",
    "    if hasattr(model, 'export'):\n",
    "        exported_model = model.export()\n",
    "        trainer.model = exported_model\n",
    "        trainer.save_model(training_args.output_dir)\n",
    "    else:\n",
    "        # Fallback: save without export\n",
    "        # The model checkpoints already contain the distilled weights\n",
    "        print(\"âš ï¸  Model doesn't have export() method\")\n",
    "        print(\"âœ… Using checkpoint-3200 as final model (already saved during training)\")\n",
    "        \n",
    "        # Copy the latest checkpoint to output directory\n",
    "        import shutil\n",
    "        checkpoint_dir = f\"{training_args.output_dir}/checkpoint-3200\"\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            # The checkpoint is already there, just note it\n",
    "            print(f\"âœ… Final model available at: {checkpoint_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Export failed: {e}\")\n",
    "    print(\"âœ… Model weights saved in checkpoints during training\")\n",
    "\n",
    "print(f\"\\nâœ… Model saved successfully!\")\n",
    "print(f\"\\nðŸ“‚ Available checkpoints:\")\n",
    "for item in sorted(os.listdir(training_args.output_dir)):\n",
    "    item_path = os.path.join(training_args.output_dir, item)\n",
    "    if os.path.isdir(item_path) and item.startswith('checkpoint'):\n",
    "        print(f\"  - {item}\")\n",
    "        \n",
    "print(f\"\\nðŸ’¡ To use the model, load from the latest checkpoint:\")\n",
    "print(f\"   AutoModelForCausalLM.from_pretrained('{training_args.output_dir}/checkpoint-3200')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 10: Comprehensive Evaluation - Before & After Distillation\n",
    "\n",
    "We'll evaluate three models to demonstrate the effectiveness of knowledge distillation:\n",
    "1. **Teacher Model** (3B params) - Our target/reference\n",
    "2. **Baseline Student** (1B params) - Before distillation (untrained)\n",
    "3. **Distilled Student** (1B params) - After distillation (trained)\n",
    "\n",
    "This comparison will show:\n",
    "- How much the baseline student lags behind the teacher\n",
    "- How much knowledge distillation helps close the gap\n",
    "- The effectiveness of our training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£  TEACHER MODEL (3B - Target Performance)\n",
      "================================================================================\n",
      "Evaluating Teacher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/kpaksaran/.venv/lib/python3.11/site-packages/modelopt/torch/distill/distillation_model.py:312: UserWarning: Teacher's Module `LlamaForCausalLM` already has an intermediate output stored. This is expected when `DistillationModel.compute_kd_loss` is not called in eval mode.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Teacher Loss: 1.6820\n",
      "âœ… Teacher Perplexity: 5.38\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£  BASELINE STUDENT (1B - Before Distillation)\n",
      "================================================================================\n",
      "Loading baseline student model (untrained)...\n",
      "\n",
      "Evaluating Baseline Student...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Baseline Loss: 2.3376\n",
      "âœ… Baseline Perplexity: 10.36\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£  DISTILLED STUDENT (1B - After Distillation)\n",
      "================================================================================\n",
      "Loading distilled model...\n",
      "\n",
      "Loading from: ./llama3.2-1b-distilled-1gpu/checkpoint-3200\n",
      "Restored ModelOpt state from ./llama3.2-1b-distilled-1gpu/checkpoint-3200/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Distilled Student...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Distilled Loss: 1.8450\n",
      "âœ… Distilled Perplexity: 6.33\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "                        Eval Loss  Perplexity  Samples/sec\n",
      "Model                                                     \n",
      "Teacher (3B)             1.682030    5.376461       21.560\n",
      "Baseline Student (1B)    2.337583   10.356179       42.929\n",
      "Distilled Student (1B)   1.844975    6.327943       43.541\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ KEY INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "1. Performance Improvement:\n",
      "   Distillation improved perplexity by 38.9%\n",
      "   (from 10.36 to 6.33)\n",
      "\n",
      "2. Gap to Teacher:\n",
      "   Baseline gap: 4.98 perplexity points\n",
      "   Distilled gap: 0.95 perplexity points\n",
      "   Knowledge distillation closed 80.9% of the gap!\n",
      "\n",
      "3. Efficiency:\n",
      "   Distilled model has 1.24B parameters\n",
      "   Teacher has 3.21B parameters\n",
      "   Achieved 80.9% of teacher quality\n",
      "   with only 38.5% of parameters!\n",
      "\n",
      "================================================================================\n",
      "âœ… Evaluation Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Helper function to evaluate a model\n",
    "def evaluate_model(model, model_name):\n",
    "    \"\"\"Evaluate a model and return metrics\"\"\"\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    eval_trainer = transformers.Trainer(\n",
    "        model,\n",
    "        transformers.TrainingArguments(\n",
    "            output_dir=training_args.output_dir,\n",
    "            per_device_eval_batch_size=2,\n",
    "            bf16=True,\n",
    "        ),\n",
    "        eval_dataset=dset_eval,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    \n",
    "    results = eval_trainer.evaluate()\n",
    "    perplexity = math.exp(results['eval_loss'])\n",
    "    \n",
    "    return {\n",
    "        'loss': results['eval_loss'],\n",
    "        'perplexity': perplexity,\n",
    "        'runtime': results['eval_runtime'],\n",
    "        'samples_per_sec': results['eval_samples_per_second']\n",
    "    }\n",
    "\n",
    "# Store results\n",
    "results_dict = {}\n",
    "\n",
    "# 1. Evaluate Teacher Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1ï¸âƒ£  TEACHER MODEL (3B - Target Performance)\")\n",
    "print(\"=\"*80)\n",
    "results_dict['Teacher (3B)'] = evaluate_model(teacher_model, \"Teacher\")\n",
    "print(f\"âœ… Teacher Loss: {results_dict['Teacher (3B)']['loss']:.4f}\")\n",
    "print(f\"âœ… Teacher Perplexity: {results_dict['Teacher (3B)']['perplexity']:.2f}\")\n",
    "\n",
    "# 2. Evaluate Baseline Student (before distillation)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2ï¸âƒ£  BASELINE STUDENT (1B - Before Distillation)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Loading baseline student model (untrained)...\\n\")\n",
    "\n",
    "baseline_student = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.student_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "\n",
    "results_dict['Baseline Student (1B)'] = evaluate_model(baseline_student, \"Baseline Student\")\n",
    "print(f\"âœ… Baseline Loss: {results_dict['Baseline Student (1B)']['loss']:.4f}\")\n",
    "print(f\"âœ… Baseline Perplexity: {results_dict['Baseline Student (1B)']['perplexity']:.2f}\")\n",
    "\n",
    "# Free memory\n",
    "del baseline_student\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3. Evaluate Distilled Student (after training)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3ï¸âƒ£  DISTILLED STUDENT (1B - After Distillation)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Loading distilled model...\\n\")\n",
    "\n",
    "# Load from the latest checkpoint\n",
    "distilled_model_path = f\"{training_args.output_dir}/checkpoint-{training_args.max_steps}\"\n",
    "if not os.path.exists(distilled_model_path):\n",
    "    # Fallback to the output directory itself\n",
    "    distilled_model_path = training_args.output_dir\n",
    "    \n",
    "print(f\"Loading from: {distilled_model_path}\")\n",
    "\n",
    "distilled_model = AutoModelForCausalLM.from_pretrained(\n",
    "    distilled_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "\n",
    "results_dict['Distilled Student (1B)'] = evaluate_model(distilled_model, \"Distilled Student\")\n",
    "print(f\"âœ… Distilled Loss: {results_dict['Distilled Student (1B)']['loss']:.4f}\")\n",
    "print(f\"âœ… Distilled Perplexity: {results_dict['Distilled Student (1B)']['perplexity']:.2f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "df = pd.DataFrame(results_dict).T\n",
    "df.index.name = 'Model'\n",
    "df = df[['loss', 'perplexity', 'samples_per_sec']]\n",
    "df.columns = ['Eval Loss', 'Perplexity', 'Samples/sec']\n",
    "\n",
    "print(\"\\n\" + df.to_string())\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_ppl = results_dict['Baseline Student (1B)']['perplexity']\n",
    "distilled_ppl = results_dict['Distilled Student (1B)']['perplexity']\n",
    "teacher_ppl = results_dict['Teacher (3B)']['perplexity']\n",
    "\n",
    "improvement = ((baseline_ppl - distilled_ppl) / baseline_ppl) * 100\n",
    "gap_closed = ((baseline_ppl - teacher_ppl) - (distilled_ppl - teacher_ppl)) / (baseline_ppl - teacher_ppl) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Performance Improvement:\")\n",
    "print(f\"   Distillation improved perplexity by {improvement:.1f}%\")\n",
    "print(f\"   (from {baseline_ppl:.2f} to {distilled_ppl:.2f})\")\n",
    "\n",
    "print(f\"\\n2. Gap to Teacher:\")\n",
    "print(f\"   Baseline gap: {baseline_ppl - teacher_ppl:.2f} perplexity points\")\n",
    "print(f\"   Distilled gap: {distilled_ppl - teacher_ppl:.2f} perplexity points\")\n",
    "print(f\"   Knowledge distillation closed {gap_closed:.1f}% of the gap!\")\n",
    "\n",
    "print(f\"\\n3. Efficiency:\")\n",
    "print(f\"   Distilled model has {student_params/1e9:.2f}B parameters\")\n",
    "print(f\"   Teacher has {teacher_params/1e9:.2f}B parameters\")\n",
    "print(f\"   Achieved {(1 - (distilled_ppl - teacher_ppl)/(baseline_ppl - teacher_ppl)) * 100:.1f}% of teacher quality\")\n",
    "print(f\"   with only {(student_params/teacher_params)*100:.1f}% of parameters!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Evaluation Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 11: Test Inference\n",
    "\n",
    "Test the distilled model with sample prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing distilled model inference...\n",
      "\n",
      "ðŸ“ Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 13 Nov 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is knowledge distillation and why is it useful?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Generating response...\n",
      "\n",
      "ðŸ¤– Response:\n",
      "================================================================================\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 13 Nov 2025\n",
      "\n",
      "user\n",
      "\n",
      "What is knowledge distillation and why is it useful?assistant\n",
      "\n",
      "Knowledge distillation is the process of selectively extracting the most valuable and relevant information from a vast amount of knowledge sources, such as books, articles, research papers, and expert opinions, and distilling it into a concise and actionable form. The goal of knowledge distillation is to identify the most important insights, patterns, and principles that can be applied to a specific problem or issue.\n",
      "\n",
      "The benefits of knowledge distillation include:\n",
      "\n",
      "1. **Reducing information overload**: By distilling complex information into a concise and actionable form, knowledge distillation helps to simplify complex ideas and make them more accessible to a wider audience.\n",
      "2. **Enhancing learning**: Distilling knowledge allows individuals to focus on the most important concepts and principles, making it easier to absorb and apply them to real-world problems.\n",
      "3. **Improving decision-making**: Knowledge distillation can help individuals make more informed decisions by providing them with a clear understanding of the key insights and findings from expert opinions and research.\n",
      "4. **Fostering\n",
      "================================================================================\n",
      "\n",
      "âœ… Inference complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing distilled model inference...\\n\")\n",
    "\n",
    "# Test prompt\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is knowledge distillation and why is it useful?\"}\n",
    "]\n",
    "\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    test_messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ“ Prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(distilled_model.device)\n",
    "\n",
    "# Generate response\n",
    "print(\"Generating response...\\n\")\n",
    "with torch.no_grad():\n",
    "    outputs = distilled_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ðŸ¤– Response:\")\n",
    "print(\"=\"*80)\n",
    "print(response)\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Lab Complete!\n",
    "\n",
    "### Summary\n",
    "\n",
    "You have successfully:\n",
    "1. âœ… Configured a single GPU environment\n",
    "2. âœ… Loaded and preprocessed a conversational dataset\n",
    "3. âœ… Set up knowledge distillation between teacher and student models\n",
    "4. âœ… Trained a distilled 1B parameter model from a 3B teacher\n",
    "5. âœ… Evaluated the distilled model's performance\n",
    "6. âœ… Tested inference with the distilled model\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Model Compression**: Reduced from 3.2B to 1.2B parameters (2.6Ã— smaller)\n",
    "- **Knowledge Transfer**: Student learned from both ground truth and teacher predictions\n",
    "- **Practical Skills**: Dataset preprocessing, distributed training setup, model evaluation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Compare Performance**: Benchmark the distilled model against the teacher\n",
    "2. **Optimize Further**: Try different hyperparameters or longer training\n",
    "3. **Deploy**: Use the smaller model for efficient inference\n",
    "4. **Experiment**: Try different teacher-student pairs or datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Happy distilling! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpaksaran",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
